#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
–ü—Ä–∞–∫—Ç–∏—á–Ω–∏–π –Ω–æ—É—Ç–±—É–∫ –¥–æ –õ–µ–∫—Ü—ñ—ó 1: –í—Å—Ç—É–ø –¥–æ –≥–ª–∏–±–æ–∫–æ–≥–æ –Ω–∞–≤—á–∞–Ω–Ω—è
–ö—É—Ä—Å: Deep Learning
–ê–≤—Ç–æ—Ä: –ó—ñ–≤–∞–∫—ñ–Ω –í–∞–ª–µ—Ä—ñ–π –î–º–∏—Ç—Ä–æ–≤–∏—á

–¶–µ–π –Ω–æ—É—Ç–±—É–∫ –≤ –º–∞–π–±—É—Ç–Ω—å–æ–º—É –º–∞—î –∑–∞–ø—É—Å–∫–∞—Ç–∏—Å—è –≤ (–Ω–µ –∑–∞–±—É–¥—å—Ç–µ –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ!):
- Google Colab (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ)
- –õ–æ–∫–∞–ª—å–Ω–æ –∑ GPU
- Kaggle Notebooks
"""

# ============================================================================
# –ß–ê–°–¢–ò–ù–ê 0: –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø –°–ï–†–ï–î–û–í–ò–©–ê
# ============================================================================

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞, —á–∏ –º–∏ –≤ Google Colab
import sys

IN_COLAB = 'google.colab' in sys.modules

if IN_COLAB:
    print("üéâ –ó–∞–ø—É—â–µ–Ω–æ –≤ Google Colab!")
    print("GPU –≤–∂–µ –º–∞—î –±—É—Ç–∏ –∞–∫—Ç–∏–≤–æ–≤–∞–Ω–∞ (Runtime -> Change runtime type -> GPU)")
else:
    print("üíª –ó–∞–ø—É—â–µ–Ω–æ –ª–æ–∫–∞–ª—å–Ω–æ")

# –û—Å–Ω–æ–≤–Ω—ñ —ñ–º–ø–æ—Ä—Ç–∏
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import time
from tqdm import tqdm
import warnings

warnings.filterwarnings('ignore')

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è matplotlib
plt.style.use('seaborn-v0_8-darkgrid')
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['font.size'] = 12


# ============================================================================
# –ß–ê–°–¢–ò–ù–ê 1: –ü–ï–†–ï–í–Ü–†–ö–ê –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø
# ============================================================================

def check_environment():
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è"""
    print("=" * 60)
    print("–Ü–ù–§–û–†–ú–ê–¶–Ü–Ø –ü–†–û –°–ï–†–ï–î–û–í–ò–©–ï")
    print("=" * 60)

    # PyTorch –≤–µ—Ä—Å—ñ—è
    print(f"üì¶ PyTorch –≤–µ—Ä—Å—ñ—è: {torch.__version__}")

    # CUDA –¥–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å
    cuda_available = torch.cuda.is_available()
    print(f"üñ•Ô∏è  CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {cuda_available}")

    if cuda_available:
        print(f"üîß CUDA –≤–µ—Ä—Å—ñ—è: {torch.version.cuda}")
        print(f"üìä –ö—ñ–ª—å–∫—ñ—Å—Ç—å GPU: {torch.cuda.device_count()}")

        for i in range(torch.cuda.device_count()):
            print(f"\nGPU {i}:")
            print(f"  –ù–∞–∑–≤–∞: {torch.cuda.get_device_name(i)}")
            props = torch.cuda.get_device_properties(i)
            print(f"  –ü–∞–º'—è—Ç—å: {props.total_memory / 1024 ** 3:.2f} GB")
            print(f"  Compute Capability: {props.major}.{props.minor}")
    else:
        print("‚ö†Ô∏è  GPU –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ CPU.")
        print("–î–ª—è –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó GPU –≤ Colab: Runtime -> Change runtime type -> GPU")

    # CPU —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è
    print(f"\nüíª CPU threads: {torch.get_num_threads()}")

    return cuda_available


# –ó–∞–ø—É—Å–∫ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
cuda_available = check_environment()

# –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è device
device = torch.device("cuda" if cuda_available else "cpu")
print(f"\n‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ device: {device}")

# ============================================================================
# –ß–ê–°–¢–ò–ù–ê 2: –û–°–ù–û–í–ò –¢–ï–ù–ó–û–†–Ü–í
# ============================================================================

print("\n" + "=" * 60)
print("–ß–ê–°–¢–ò–ù–ê 2: –û–°–ù–û–í–ò –¢–ï–ù–ó–û–†–Ü–í")
print("=" * 60)


# 2.1 –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–µ–Ω–∑–æ—Ä—ñ–≤ —Ä—ñ–∑–Ω–∏–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏
def tensor_creation_demo():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–µ–Ω–∑–æ—Ä—ñ–≤"""
    print("\nüìù –°–ø–æ—Å–æ–±–∏ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–µ–Ω–∑–æ—Ä—ñ–≤:\n")

    # –ó Python —Å–ø–∏—Å–∫—É
    tensor_list = torch.tensor([1, 2, 3, 4, 5])
    print(f"–ó —Å–ø–∏—Å–∫—É: {tensor_list}")

    # –ó NumPy
    numpy_array = np.array([[1, 2], [3, 4]])
    tensor_numpy = torch.from_numpy(numpy_array)
    print(f"–ó NumPy: {tensor_numpy}")

    # –°–ø–µ—Ü—ñ–∞–ª—å–Ω—ñ —Ç–µ–Ω–∑–æ—Ä–∏
    zeros = torch.zeros(2, 3)
    ones = torch.ones(2, 3)
    rand = torch.rand(2, 3)  # [0, 1)
    randn = torch.randn(2, 3)  # N(0, 1)

    print(f"\n–ù—É–ª—ñ:\n{zeros}")
    print(f"\n–í–∏–ø–∞–¥–∫–æ–≤—ñ [0,1):\n{rand}")
    print(f"\n–ù–æ—Ä–º–∞–ª—å–Ω–∏–π —Ä–æ–∑–ø–æ–¥—ñ–ª:\n{randn}")

    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –Ω–∞ GPU
    if cuda_available:
        gpu_tensor = torch.randn(2, 3, device='cuda')
        print(f"\n–¢–µ–Ω–∑–æ—Ä –Ω–∞ GPU: {gpu_tensor.device}")


tensor_creation_demo()


# 2.2 –û–ø–µ—Ä–∞—Ü—ñ—ó –∑ —Ç–µ–Ω–∑–æ—Ä–∞–º–∏
def tensor_operations_demo():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—è –æ–ø–µ—Ä–∞—Ü—ñ–π –∑ —Ç–µ–Ω–∑–æ—Ä–∞–º–∏"""
    print("\nüìä –û–ø–µ—Ä–∞—Ü—ñ—ó –∑ —Ç–µ–Ω–∑–æ—Ä–∞–º–∏:\n")

    # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–µ–Ω–∑–æ—Ä–∏
    a = torch.tensor([[1., 2.], [3., 4.]])
    b = torch.tensor([[5., 6.], [7., 8.]])

    print(f"–¢–µ–Ω–∑–æ—Ä A:\n{a}")
    print(f"–¢–µ–Ω–∑–æ—Ä B:\n{b}")

    # –ê—Ä–∏—Ñ–º–µ—Ç–∏—á–Ω—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó
    print(f"\nA + B:\n{a + b}")
    print(f"\nA * B (element-wise):\n{a * b}")
    print(f"\nA @ B (matrix mult):\n{a @ b}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω—ñ –æ–ø–µ—Ä–∞—Ü—ñ—ó
    c = torch.randn(3, 4)
    print(f"\n–í–∏–ø–∞–¥–∫–æ–≤–∏–π —Ç–µ–Ω–∑–æ—Ä C:\n{c}")
    print(f"Mean: {c.mean():.4f}")
    print(f"Std: {c.std():.4f}")
    print(f"Max: {c.max():.4f}")
    print(f"Min: {c.min():.4f}")

    # –ó–º—ñ–Ω–∞ —Ñ–æ—Ä–º–∏
    d = torch.arange(12)
    print(f"\n–í–∏—Ö—ñ–¥–Ω–∏–π: {d}")
    print(f"Reshape (3,4): {d.reshape(3, 4)}")
    print(f"View (2,6): {d.view(2, 6)}")


tensor_operations_demo()


# 2.3 Broadcasting
def broadcasting_demo():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—è broadcasting"""
    print("\nüì° Broadcasting:\n")

    # –í–µ–∫—Ç–æ—Ä + —Å–∫–∞–ª—è—Ä
    vec = torch.tensor([1, 2, 3])
    scalar = 10
    print(f"–í–µ–∫—Ç–æ—Ä {vec} + —Å–∫–∞–ª—è—Ä {scalar} = {vec + scalar}")

    # –ú–∞—Ç—Ä–∏—Ü—è + –≤–µ–∫—Ç–æ—Ä
    matrix = torch.randn(3, 4)
    row_vec = torch.randn(4)
    col_vec = torch.randn(3, 1)

    print(f"\n–ú–∞—Ç—Ä–∏—Ü—è shape: {matrix.shape}")
    print(f"–†—è–¥–æ–∫ shape: {row_vec.shape}")
    print(f"–°—Ç–æ–≤–ø–µ—Ü—å shape: {col_vec.shape}")

    result1 = matrix + row_vec  # Broadcasting –ø–æ —Ä—è–¥–∫–∞—Ö
    result2 = matrix + col_vec  # Broadcasting –ø–æ —Å—Ç–æ–≤–ø—Ü—è—Ö

    print(f"–ú–∞—Ç—Ä–∏—Ü—è + —Ä—è–¥–æ–∫: {result1.shape}")
    print(f"–ú–∞—Ç—Ä–∏—Ü—è + —Å—Ç–æ–≤–ø–µ—Ü—å: {result2.shape}")


broadcasting_demo()

# ============================================================================
# –ß–ê–°–¢–ò–ù–ê 3: CPU vs GPU BENCHMARK
# ============================================================================

print("\n" + "=" * 60)
print("–ß–ê–°–¢–ò–ù–ê 3: –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø CPU vs GPU")
print("=" * 60)


def benchmark_device(size, device, num_iterations=100):
    """Benchmark –æ–ø–µ—Ä–∞—Ü—ñ–π –Ω–∞ –∑–∞–¥–∞–Ω–æ–º—É device"""

    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–µ–Ω–∑–æ—Ä—ñ–≤ –Ω–∞ device
    a = torch.randn(size, size, device=device)
    b = torch.randn(size, size, device=device)

    # –†–æ–∑—ñ–≥—Ä—ñ–≤
    for _ in range(10):
        c = a @ b
        if device == 'cuda':
            torch.cuda.synchronize()

    # –í–∏–º—ñ—Ä—é–≤–∞–Ω–Ω—è —á–∞—Å—É
    start_time = time.time()

    for _ in range(num_iterations):
        c = torch.matmul(a, b)
        if device == 'cuda':
            torch.cuda.synchronize()

    elapsed_time = time.time() - start_time
    avg_time = elapsed_time / num_iterations

    return avg_time


def compare_devices():
    """–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ CPU —Ç–∞ GPU"""

    if not cuda_available:
        print("‚ö†Ô∏è  GPU –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ benchmark.")
        return

    sizes = [128, 256, 512, 1024, 2048]
    cpu_times = []
    gpu_times = []

    print("\n‚è±Ô∏è  –ó–∞–ø—É—Å–∫ benchmark (—Ü–µ –º–æ–∂–µ –∑–∞–π–Ω—è—Ç–∏ —Ö–≤–∏–ª–∏–Ω—É)...\n")

    for size in sizes:
        print(f"Testing size {size}x{size}...")

        # CPU benchmark
        cpu_time = benchmark_device(size, 'cpu', num_iterations=10)
        cpu_times.append(cpu_time)

        # GPU benchmark
        gpu_time = benchmark_device(size, 'cuda', num_iterations=10)
        gpu_times.append(gpu_time)

        speedup = cpu_time / gpu_time
        print(f"  CPU: {cpu_time:.4f}s, GPU: {gpu_time:.4f}s, Speedup: {speedup:.1f}x")

    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    plt.figure(figsize=(12, 5))

    # –ì—Ä–∞—Ñ—ñ–∫ —á–∞—Å—É
    plt.subplot(1, 2, 1)
    plt.plot(sizes, cpu_times, 'b-o', label='CPU', linewidth=2, markersize=8)
    plt.plot(sizes, gpu_times, 'r-o', label='GPU', linewidth=2, markersize=8)
    plt.xlabel('Matrix Size')
    plt.ylabel('Time (seconds)')
    plt.title('Matrix Multiplication Performance')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.yscale('log')

    # –ì—Ä–∞—Ñ—ñ–∫ –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è
    plt.subplot(1, 2, 2)
    speedups = [c / g for c, g in zip(cpu_times, gpu_times)]
    plt.bar(range(len(sizes)), speedups, color='green', alpha=0.7)
    plt.xticks(range(len(sizes)), sizes)
    plt.xlabel('Matrix Size')
    plt.ylabel('Speedup (x times)')
    plt.title('GPU Speedup over CPU')
    plt.grid(True, alpha=0.3)

    # –î–æ–¥–∞—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –Ω–∞ —Å—Ç–æ–≤–ø—á–∏–∫–∏
    for i, (size, speedup) in enumerate(zip(sizes, speedups)):
        plt.text(i, speedup + 0.5, f'{speedup:.1f}x',
                 ha='center', fontweight='bold')

    plt.tight_layout()
    plt.show()


compare_devices()

# ============================================================================
# –ß–ê–°–¢–ò–ù–ê 4: –ü–ï–†–®–ê –ù–ï–ô–†–û–ù–ù–ê –ú–ï–†–ï–ñ–ê
# ============================================================================

print("\n" + "=" * 60)
print("–ß–ê–°–¢–ò–ù–ê 4: –ü–ï–†–®–ê –õ–Ü–ù–Ü–ô–ù–ê –ú–û–î–ï–õ–¨")
print("=" * 60)


# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Å–∏–Ω—Ç–µ—Ç–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö
def generate_synthetic_data(n_samples=1000, n_features=10, noise_level=0.1):
    """–ì–µ–Ω–µ—Ä—É—î –¥–∞–Ω—ñ –¥–ª—è –ª—ñ–Ω—ñ–π–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó: y = Xw + b + noise"""

    torch.manual_seed(42)

    # –°–ø—Ä–∞–≤–∂–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
    true_weights = torch.randn(n_features, 1) * 2
    true_bias = torch.randn(1) * 0.5

    # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö
    X = torch.randn(n_samples, n_features)
    noise = torch.randn(n_samples, 1) * noise_level
    y = X @ true_weights + true_bias + noise

    return X, y, true_weights, true_bias


# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤
print("üìä –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Å–∏–Ω—Ç–µ—Ç–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö...")
X_train, y_train, true_w, true_b = generate_synthetic_data(5000, 20, 0.1)
X_val, y_val, _, _ = generate_synthetic_data(1000, 20, 0.1)

print(f"–¢—Ä–µ–Ω—É–≤–∞–ª—å–Ω—ñ –¥–∞–Ω—ñ: X={X_train.shape}, y={y_train.shape}")
print(f"–í–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω—ñ –¥–∞–Ω—ñ: X={X_val.shape}, y={y_val.shape}")
print(f"–°–ø—Ä–∞–≤–∂–Ω—ñ –≤–∞–≥–∏ (–ø–µ—Ä—à—ñ 5): {true_w[:5].squeeze().tolist()}")
print(f"–°–ø—Ä–∞–≤–∂–Ω—î –∑–º—ñ—â–µ–Ω–Ω—è: {true_b.item():.4f}")

# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è DataLoader
train_dataset = TensorDataset(X_train, y_train)
val_dataset = TensorDataset(X_val, y_val)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)


# –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
class LinearRegression(nn.Module):
    def __init__(self, input_dim):
        super(LinearRegression, self).__init__()
        self.linear = nn.Linear(input_dim, 1)

    def forward(self, x):
        return self.linear(x)


# –§—É–Ω–∫—Ü—ñ—è –Ω–∞–≤—á–∞–Ω–Ω—è
def train_epoch(model, dataloader, criterion, optimizer, device):
    """–û–¥–Ω–∞ –µ–ø–æ—Ö–∞ –Ω–∞–≤—á–∞–Ω–Ω—è"""
    model.train()
    total_loss = 0

    for batch_x, batch_y in dataloader:
        batch_x = batch_x.to(device)
        batch_y = batch_y.to(device)

        # Forward pass
        predictions = model(batch_x)
        loss = criterion(predictions, batch_y)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * batch_x.size(0)

    return total_loss / len(dataloader.dataset)


def validate_epoch(model, dataloader, criterion, device):
    """–í–∞–ª—ñ–¥–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ"""
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for batch_x, batch_y in dataloader:
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)

            predictions = model(batch_x)
            loss = criterion(predictions, batch_y)

            total_loss += loss.item() * batch_x.size(0)

    return total_loss / len(dataloader.dataset)


# –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
def train_model(device='cpu', epochs=50):
    """–ü–æ–≤–Ω–∏–π —Ü–∏–∫–ª –Ω–∞–≤—á–∞–Ω–Ω—è"""
    print(f"\nüöÄ –ù–∞–≤—á–∞–Ω–Ω—è –Ω–∞ {device.upper()}...")

    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è
    model = LinearRegression(20).to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.01)

    train_losses = []
    val_losses = []

    # Progress bar
    pbar = tqdm(range(epochs), desc='Training')

    start_time = time.time()

    for epoch in pbar:
        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
        val_loss = validate_epoch(model, val_loader, criterion, device)

        train_losses.append(train_loss)
        val_losses.append(val_loss)

        # –û–Ω–æ–≤–ª–µ–Ω–Ω—è progress bar
        pbar.set_postfix({
            'train_loss': f'{train_loss:.4f}',
            'val_loss': f'{val_loss:.4f}'
        })

    training_time = time.time() - start_time

    print(f"‚úÖ –ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {training_time:.2f} —Å–µ–∫—É–Ω–¥")
    print(f"üìâ –§—ñ–Ω–∞–ª—å–Ω—ñ –≤—Ç—Ä–∞—Ç–∏ - Train: {train_losses[-1]:.4f}, Val: {val_losses[-1]:.4f}")

    return model, train_losses, val_losses, training_time


# –ù–∞–≤—á–∞–Ω–Ω—è –Ω–∞ CPU
cpu_model, cpu_train_losses, cpu_val_losses, cpu_time = train_model('cpu', epochs=30)

# –ù–∞–≤—á–∞–Ω–Ω—è –Ω–∞ GPU (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–æ)
if cuda_available:
    gpu_model, gpu_train_losses, gpu_val_losses, gpu_time = train_model('cuda', epochs=30)
    print(f"\n‚ö° –ü—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è GPU: {cpu_time / gpu_time:.2f}x")
else:
    gpu_train_losses = None
    gpu_val_losses = None

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –Ω–∞–≤—á–∞–Ω–Ω—è
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(cpu_train_losses, 'b-', label='CPU Train', linewidth=2)
plt.plot(cpu_val_losses, 'b--', label='CPU Val', linewidth=2)

if gpu_train_losses:
    plt.plot(gpu_train_losses, 'r-', label='GPU Train', linewidth=2)
    plt.plot(gpu_val_losses, 'r--', label='GPU Val', linewidth=2)

plt.xlabel('Epoch')
plt.ylabel('MSE Loss')
plt.title('Training Progress')
plt.legend()
plt.grid(True, alpha=0.3)

# –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —Å–ø—Ä–∞–≤–∂–Ω—ñ–º–∏ –≤–∞–≥–∞–º–∏
plt.subplot(1, 2, 2)
cpu_weights = cpu_model.linear.weight.detach().cpu().numpy().squeeze()
true_weights = true_w.numpy().squeeze()

x = np.arange(len(cpu_weights))
width = 0.35

plt.bar(x - width / 2, true_weights, width, label='True Weights', color='green', alpha=0.7)
plt.bar(x + width / 2, cpu_weights, width, label='Learned Weights', color='blue', alpha=0.7)

plt.xlabel('Weight Index')
plt.ylabel('Weight Value')
plt.title('True vs Learned Weights')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ============================================================================
# –ß–ê–°–¢–ò–ù–ê 5: –ü–†–ê–ö–¢–ò–ß–ù–Ü –í–ü–†–ê–í–ò
# ============================================================================

print("\n" + "=" * 60)
print("–ü–†–ê–ö–¢–ò–ß–ù–Ü –í–ü–†–ê–í–ò")
print("=" * 60)

print("""
 –î–∏–≤–∏—Ç–∏—Å—å 1_tasks.ipynb
""")


# ============================================================================
# –ß–ê–°–¢–ò–ù–ê 6: –î–û–î–ê–¢–ö–û–í–Ü –ö–û–†–ò–°–ù–Ü –§–£–ù–ö–¶–Ü–á
# ============================================================================

def gpu_memory_summary():
    """–í–∏–≤–æ–¥–∏—Ç—å —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –ø–∞–º'—è—Ç—ñ GPU"""
    if not cuda_available:
        print("GPU –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
        return

    print("\n" + "=" * 60)
    print("GPU MEMORY SUMMARY")
    print("=" * 60)

    for i in range(torch.cuda.device_count()):
        print(f"\nGPU {i}: {torch.cuda.get_device_name(i)}")
        print(f"  Allocated: {torch.cuda.memory_allocated(i) / 1024 ** 2:.2f} MB")
        print(f"  Cached: {torch.cuda.memory_reserved(i) / 1024 ** 2:.2f} MB")

        # –û—á–∏—â–µ–Ω–Ω—è –∫–µ—à—É
        torch.cuda.empty_cache()
        print(f"  After cache clear: {torch.cuda.memory_reserved(i) / 1024 ** 2:.2f} MB")


gpu_memory_summary()

# ============================================================================
# –ó–ë–ï–†–ï–ñ–ï–ù–ù–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–Ü–í
# ============================================================================

print("\n" + "=" * 60)
print("–ó–ë–ï–†–ï–ñ–ï–ù–ù–Ø –ú–û–î–ï–õ–Ü")
print("=" * 60)

# –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
model_path = 'linear_model.pth'
torch.save({
    'model_state_dict': cpu_model.state_dict(),
    'train_losses': cpu_train_losses,
    'val_losses': cpu_val_losses
}, model_path)

print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–∞ –≤ {model_path}")

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
loaded_model = LinearRegression(20)
checkpoint = torch.load(model_path, map_location=device)
loaded_model.load_state_dict(checkpoint['model_state_dict'])
loaded_model.eval()

print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞")

# –§—ñ–Ω–∞–ª—å–Ω–µ —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
with torch.no_grad():
    test_input = torch.randn(5, 20)
    test_output = loaded_model(test_input)
    print(f"\n–¢–µ—Å—Ç –º–æ–¥–µ–ª—ñ - –≤—Ö—ñ–¥: {test_input.shape}, –≤–∏—Ö—ñ–¥: {test_output.shape}")

print("\n" + "=" * 60)
print("üéâ –ù–û–£–¢–ë–£–ö –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–Ü–®–ù–û!")
print("=" * 60)
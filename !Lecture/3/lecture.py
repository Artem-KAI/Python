#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
–ü—Ä–∞–∫—Ç–∏—á–Ω–∏–π –Ω–æ—É—Ç–±—É–∫ –¥–æ –õ–µ–∫—Ü—ñ—ó 3: –ù–∞–≤—á–∞–Ω–Ω—è –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂
–ö—É—Ä—Å: Deep Learning
"""

# ============================================================================
# –ß–ê–°–¢–ò–ù–ê 0: –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø
# ============================================================================

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns
from tqdm import tqdm

# –î–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
from IPython.display import HTML
import matplotlib.animation as animation

import warnings

warnings.filterwarnings('ignore')

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
plt.style.use('seaborn-v0_8-darkgrid')
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ device: {device}")

# Seed –¥–ª—è –≤—ñ–¥—Ç–≤–æ—Ä—é–≤–∞–Ω–æ—Å—Ç—ñ
torch.manual_seed(42)
np.random.seed(42)

# ============================================================================
# –ß–ê–°–¢–ò–ù–ê 1: BACKPROPAGATION - –†–£–ß–ù–ê –†–ï–ê–õ–Ü–ó–ê–¶–Ü–Ø
# ============================================================================

print("=" * 60)
print("–ß–ê–°–¢–ò–ù–ê 1: BACKPROPAGATION –ö–†–û–ö –ó–ê –ö–†–û–ö–û–ú")
print("=" * 60)


class SimpleNetwork:
    """–ü—Ä–æ—Å—Ç–∞ 2-—à–∞—Ä–æ–≤–∞ –º–µ—Ä–µ–∂–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó backprop"""

    def __init__(self, input_size=2, hidden_size=3, output_size=1):
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –≤–∞–≥
        self.W1 = np.random.randn(input_size, hidden_size) * 0.5
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * 0.5
        self.b2 = np.zeros((1, output_size))

        # –î–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—Ä–æ–º—ñ–∂–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å
        self.cache = {}

    def relu(self, z):
        """ReLU –∞–∫—Ç–∏–≤–∞—Ü—ñ—è"""
        return np.maximum(0, z)

    def relu_derivative(self, z):
        """–ü–æ—Ö—ñ–¥–Ω–∞ ReLU"""
        return (z > 0).astype(float)

    def forward(self, X):
        """
        Forward pass –∑ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è–º –≤—Å—ñ—Ö –ø—Ä–æ–º—ñ–∂–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å
        –¥–ª—è backpropagation
        """
        # –®–∞—Ä 1
        self.cache['X'] = X
        self.cache['z1'] = X @ self.W1 + self.b1
        self.cache['a1'] = self.relu(self.cache['z1'])

        # –®–∞—Ä 2
        self.cache['z2'] = self.cache['a1'] @ self.W2 + self.b2
        self.cache['a2'] = self.cache['z2']  # –ë–µ–∑ –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –Ω–∞ –≤–∏—Ö–æ–¥—ñ

        return self.cache['a2']

    def compute_loss(self, y_pred, y_true):
        """MSE loss"""
        m = y_true.shape[0]
        loss = np.mean((y_pred - y_true) ** 2)
        return loss

    def backward(self, y_true, learning_rate=0.01, verbose=False):
        """
        Backpropagation –∑ –¥–µ—Ç–∞–ª—å–Ω–∏–º –≤–∏–≤–µ–¥–µ–Ω–Ω—è–º
        """
        m = y_true.shape[0]

        # –ì—Ä–∞–¥—ñ—î–Ω—Ç –ø–æ –≤–∏—Ö–æ–¥—É (MSE loss)
        dL_da2 = 2 * (self.cache['a2'] - y_true) / m

        if verbose:
            print("\nüìä BACKPROPAGATION –ö–†–û–ö –ó–ê –ö–†–û–ö–û–ú:")
            print(f"1. –ì—Ä–∞–¥—ñ—î–Ω—Ç –≤—Ç—Ä–∞—Ç –ø–æ –≤–∏—Ö–æ–¥—É: dL/da2 shape = {dL_da2.shape}")

        # –ì—Ä–∞–¥—ñ—î–Ω—Ç–∏ –¥–ª—è —à–∞—Ä—É 2
        dL_dW2 = self.cache['a1'].T @ dL_da2
        dL_db2 = np.sum(dL_da2, axis=0, keepdims=True)

        if verbose:
            print(f"2. –ì—Ä–∞–¥—ñ—î–Ω—Ç –ø–æ W2: dL/dW2 shape = {dL_dW2.shape}")
            print(f"   –ì—Ä–∞–¥—ñ—î–Ω—Ç –ø–æ b2: dL/db2 shape = {dL_db2.shape}")

        # –ì—Ä–∞–¥—ñ—î–Ω—Ç —á–µ—Ä–µ–∑ —à–∞—Ä 2
        dL_da1 = dL_da2 @ self.W2.T

        if verbose:
            print(f"3. –ì—Ä–∞–¥—ñ—î–Ω—Ç –ø—Ä–æ–ø–∞–≥—É—î—Ç—å—Å—è –Ω–∞ —à–∞—Ä 1: dL/da1 shape = {dL_da1.shape}")

        # –ì—Ä–∞–¥—ñ—î–Ω—Ç —á–µ—Ä–µ–∑ ReLU
        dL_dz1 = dL_da1 * self.relu_derivative(self.cache['z1'])

        if verbose:
            print(f"4. –ì—Ä–∞–¥—ñ—î–Ω—Ç —á–µ—Ä–µ–∑ ReLU: dL/dz1 shape = {dL_dz1.shape}")

        # –ì—Ä–∞–¥—ñ—î–Ω—Ç–∏ –¥–ª—è —à–∞—Ä—É 1
        dL_dW1 = self.cache['X'].T @ dL_dz1
        dL_db1 = np.sum(dL_dz1, axis=0, keepdims=True)

        if verbose:
            print(f"5. –ì—Ä–∞–¥—ñ—î–Ω—Ç –ø–æ W1: dL/dW1 shape = {dL_dW1.shape}")
            print(f"   –ì—Ä–∞–¥—ñ—î–Ω—Ç –ø–æ b1: dL/db1 shape = {dL_db1.shape}")

        # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –≤–∞–≥
        self.W2 -= learning_rate * dL_dW2
        self.b2 -= learning_rate * dL_db2
        self.W1 -= learning_rate * dL_dW1
        self.b1 -= learning_rate * dL_db1

        # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
        return {
            'dW1': dL_dW1, 'db1': dL_db1,
            'dW2': dL_dW2, 'db2': dL_db2
        }


# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—è backpropagation
def demo_backpropagation():
    """–ü–æ–∫—Ä–æ–∫–æ–≤–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—è backprop"""

    print("\nüéØ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—è Backpropagation –Ω–∞ –ø—Ä–æ—Å—Ç–æ–º—É –ø—Ä–∏–∫–ª–∞–¥—ñ")

    # –°—Ç–≤–æ—Ä—é—î–º–æ –ø—Ä–æ—Å—Ç—É –∑–∞–¥–∞—á—É
    X = np.array([[0.5, 0.3], [0.2, 0.8], [0.9, 0.1], [0.4, 0.6]])
    y = np.array([[0.8], [1.0], [0.7], [0.9]])

    print(f"–í—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ: {X.shape}")
    print(f"–¶—ñ–ª—å–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è: {y.shape}")

    # –°—Ç–≤–æ—Ä—é—î–º–æ –º–µ—Ä–µ–∂—É
    net = SimpleNetwork(input_size=2, hidden_size=3, output_size=1)

    # Forward pass
    print("\n‚û°Ô∏è FORWARD PASS:")
    y_pred = net.forward(X)
    loss = net.compute_loss(y_pred, y)
    print(f"–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è: {y_pred.flatten()}")
    print(f"–ü–æ—á–∞—Ç–∫–æ–≤–∞ –≤—Ç—Ä–∞—Ç–∞: {loss:.4f}")

    # Backward pass –∑ –¥–µ—Ç–∞–ª—å–Ω–∏–º –≤–∏–≤–µ–¥–µ–Ω–Ω—è–º
    print("\n‚¨ÖÔ∏è BACKWARD PASS:")
    gradients = net.backward(y, learning_rate=0.1, verbose=True)

    # –ü–æ–∫–∞–∑—É—î–º–æ –≤–µ–ª–∏—á–∏–Ω–∏ –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤
    print("\nüìà –í–µ–ª–∏—á–∏–Ω–∏ –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤:")
    for name, grad in gradients.items():
        print(f"{name}: mean={np.mean(np.abs(grad)):.6f}, max={np.max(np.abs(grad)):.6f}")

    # –ù–æ–≤–∏–π forward pass –ø—ñ—Å–ª—è –æ–Ω–æ–≤–ª–µ–Ω–Ω—è
    y_pred_new = net.forward(X)
    loss_new = net.compute_loss(y_pred_new, y)
    print(f"\n‚úÖ –í—Ç—Ä–∞—Ç–∞ –ø—ñ—Å–ª—è –æ–Ω–æ–≤–ª–µ–Ω–Ω—è: {loss_new:.4f} (–∑–º–µ–Ω—à–µ–Ω–Ω—è –Ω–∞ {loss - loss_new:.4f})")


demo_backpropagation()

# ============================================================================
# –ß–ê–°–¢–ò–ù–ê 2: –§–£–ù–ö–¶–Ü–á –í–¢–†–ê–¢ - –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø
# ============================================================================

print("\n" + "=" * 60)
print("–ß–ê–°–¢–ò–ù–ê 2: –§–£–ù–ö–¶–Ü–á –í–¢–†–ê–¢")
print("=" * 60)


def visualize_loss_functions():
    """–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä—ñ–∑–Ω–∏—Ö —Ñ—É–Ω–∫—Ü—ñ–π –≤—Ç—Ä–∞—Ç"""

    fig, axes = plt.subplots(2, 3, figsize=(15, 10))

    # 1. MSE –¥–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó
    y_true = 0
    y_pred = np.linspace(-3, 3, 100)
    mse = (y_pred - y_true) ** 2
    mse_grad = 2 * (y_pred - y_true)

    axes[0, 0].plot(y_pred, mse, 'b-', linewidth=2, label='MSE')
    axes[0, 0].plot(y_pred, mse_grad, 'r--', linewidth=2, label='Gradient')
    axes[0, 0].axvline(x=y_true, color='g', linestyle=':', label='True value')
    axes[0, 0].set_title('MSE Loss')
    axes[0, 0].set_xlabel('Prediction')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # 2. MAE –¥–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó
    mae = np.abs(y_pred - y_true)
    mae_grad = np.sign(y_pred - y_true)

    axes[0, 1].plot(y_pred, mae, 'b-', linewidth=2, label='MAE')
    axes[0, 1].plot(y_pred, mae_grad, 'r--', linewidth=2, label='Gradient')
    axes[0, 1].axvline(x=y_true, color='g', linestyle=':', label='True value')
    axes[0, 1].set_title('MAE Loss')
    axes[0, 1].set_xlabel('Prediction')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # 3. Huber Loss
    delta = 1.0
    huber = np.where(np.abs(y_pred - y_true) <= delta,
                     0.5 * (y_pred - y_true) ** 2,
                     delta * (np.abs(y_pred - y_true) - 0.5 * delta))

    axes[0, 2].plot(y_pred, huber, 'b-', linewidth=2, label=f'Huber (Œ¥={delta})')
    axes[0, 2].plot(y_pred, mse, 'g--', alpha=0.5, label='MSE')
    axes[0, 2].plot(y_pred, mae, 'r--', alpha=0.5, label='MAE')
    axes[0, 2].set_title('Huber Loss (MSE + MAE hybrid)')
    axes[0, 2].set_xlabel('Prediction')
    axes[0, 2].set_ylabel('Loss')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)

    # 4. Binary Cross-Entropy
    y_true_class = 1
    y_pred_prob = np.linspace(0.01, 0.99, 100)
    bce = -y_true_class * np.log(y_pred_prob) - (1 - y_true_class) * np.log(1 - y_pred_prob)
    bce_grad = -y_true_class / y_pred_prob + (1 - y_true_class) / (1 - y_pred_prob)

    axes[1, 0].plot(y_pred_prob, bce, 'b-', linewidth=2, label='BCE (y_true=1)')
    axes[1, 0].set_title('Binary Cross-Entropy')
    axes[1, 0].set_xlabel('Predicted Probability')
    axes[1, 0].set_ylabel('Loss')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    # 5. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è BCE vs MSE –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
    axes[1, 1].plot(y_pred_prob, bce, 'b-', linewidth=2, label='BCE')
    axes[1, 1].plot(y_pred_prob, (y_pred_prob - y_true_class) ** 2, 'r-', linewidth=2, label='MSE')
    axes[1, 1].set_title('BCE vs MSE –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó')
    axes[1, 1].set_xlabel('Predicted Probability')
    axes[1, 1].set_ylabel('Loss')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)

    # 6. Focal Loss (–¥–ª—è imbalanced datasets)
    gamma = 2
    focal = -y_true_class * (1 - y_pred_prob) ** gamma * np.log(y_pred_prob)

    axes[1, 2].plot(y_pred_prob, bce, 'b-', linewidth=2, label='BCE')
    axes[1, 2].plot(y_pred_prob, focal, 'r-', linewidth=2, label=f'Focal (Œ≥={gamma})')
    axes[1, 2].set_title('Focal Loss (–¥–ª—è –Ω–µ–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö)')
    axes[1, 2].set_xlabel('Predicted Probability')
    axes[1, 2].set_ylabel('Loss')
    axes[1, 2].legend()
    axes[1, 2].grid(True, alpha=0.3)

    plt.suptitle('–§—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –∑–∞–¥–∞—á', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()


visualize_loss_functions()

# ============================================================================
# –ß–ê–°–¢–ò–ù–ê 3: –û–ü–¢–ò–ú–Ü–ó–ê–¢–û–†–ò - –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø
# ============================================================================

print("\n" + "=" * 60)
print("–ß–ê–°–¢–ò–ù–ê 3: –ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –û–ü–¢–ò–ú–Ü–ó–ê–¢–û–†–Ü–í")
print("=" * 60)


def create_loss_landscape():
    """–°—Ç–≤–æ—Ä—é—î —Å–∫–ª–∞–¥–Ω–∏–π –ª–∞–Ω–¥—à–∞—Ñ—Ç –≤—Ç—Ä–∞—Ç –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó"""
    x = np.linspace(-4, 4, 100)
    y = np.linspace(-4, 4, 100)
    X, Y = np.meshgrid(x, y)

    # Rosenbrock function - –∫–ª–∞—Å–∏—á–Ω–∞ —Ç–µ—Å—Ç–æ–≤–∞ —Ñ—É–Ω–∫—Ü—ñ—è
    Z = (1 - X) ** 2 + 100 * (Y - X ** 2) ** 2

    # –î–æ–¥–∞—î–º–æ –ª–æ–∫–∞–ª—å–Ω—ñ –º—ñ–Ω—ñ–º—É–º–∏
    Z += 5 * np.sin(2 * X) * np.sin(2 * Y)

    return X, Y, Z


def optimize_2d_function(optimizer_class, optimizer_params, start_point, num_steps=100):
    """–û–ø—Ç–∏–º—ñ–∑—É—î 2D —Ñ—É–Ω–∫—Ü—ñ—é –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –∑–∞–¥–∞–Ω–æ–≥–æ –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∞"""

    # –ü–æ—á–∞—Ç–∫–æ–≤–∞ —Ç–æ—á–∫–∞
    params = [torch.tensor([start_point[0]], dtype=torch.float32, requires_grad=True),
              torch.tensor([start_point[1]], dtype=torch.float32, requires_grad=True)]

    # –°—Ç–≤–æ—Ä—é—î–º–æ –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä
    optimizer = optimizer_class(params, **optimizer_params)

    trajectory = []
    losses = []

    for step in range(num_steps):
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ–∑–∏—Ü—ñ—é
        trajectory.append([params[0].item(), params[1].item()])

        # –û–±—á–∏—Å–ª—é—î–º–æ –≤—Ç—Ä–∞—Ç–∏ (Rosenbrock + sin)
        loss = (1 - params[0]) ** 2 + 100 * (params[1] - params[0] ** 2) ** 2
        loss += 5 * torch.sin(2 * params[0]) * torch.sin(2 * params[1])
        losses.append(loss.item())

        # Backward pass
        optimizer.zero_grad()
        loss.backward()

        # –û–Ω–æ–≤–ª–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
        optimizer.step()

    return np.array(trajectory), losses


def compare_optimizers():
    """–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç—Ä–∞—î–∫—Ç–æ—Ä—ñ–π —Ä—ñ–∑–Ω–∏—Ö –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä—ñ–≤"""

    # –°—Ç–≤–æ—Ä—é—î–º–æ –ª–∞–Ω–¥—à–∞—Ñ—Ç
    X, Y, Z = create_loss_landscape()

    # –ü–æ—á–∞—Ç–∫–æ–≤–∞ —Ç–æ—á–∫–∞
    start_point = [-2.0, 2.0]

    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä—ñ–≤
    optimizers = {
        'SGD': (optim.SGD, {'lr': 0.001}),
        'SGD + Momentum': (optim.SGD, {'lr': 0.001, 'momentum': 0.9}),
        'RMSprop': (optim.RMSprop, {'lr': 0.01}),
        'Adam': (optim.Adam, {'lr': 0.1}),
        'AdaGrad': (optim.Adagrad, {'lr': 0.5}),
    }

    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    fig = plt.figure(figsize=(18, 12))

    for idx, (name, (opt_class, opt_params)) in enumerate(optimizers.items()):
        # –¢—Ä–∞—î–∫—Ç–æ—Ä—ñ—è
        ax = plt.subplot(2, 3, idx + 1)

        # –ö–æ–Ω—Ç—É—Ä–Ω–∏–π –≥—Ä–∞—Ñ—ñ–∫
        contour = ax.contour(X, Y, Z, levels=30, alpha=0.4, cmap='viridis')
        ax.clabel(contour, inline=True, fontsize=8)

        # –û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è
        trajectory, losses = optimize_2d_function(opt_class, opt_params, start_point, num_steps=50)

        # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç—Ä–∞—î–∫—Ç–æ—Ä—ñ—ó
        ax.plot(trajectory[:, 0], trajectory[:, 1], 'r.-', linewidth=2, markersize=4)
        ax.plot(start_point[0], start_point[1], 'go', markersize=10, label='Start')
        ax.plot(trajectory[-1, 0], trajectory[-1, 1], 'r*', markersize=15, label='End')

        ax.set_title(f'{name}\nFinal loss: {losses[-1]:.2f}')
        ax.set_xlabel('x')
        ax.set_ylabel('y')
        ax.legend()
        ax.grid(True, alpha=0.3)

    # Loss curves
    ax = plt.subplot(2, 3, 6)

    for name, (opt_class, opt_params) in optimizers.items():
        _, losses = optimize_2d_function(opt_class, opt_params, start_point, num_steps=50)
        ax.plot(losses, label=name, linewidth=2)

    ax.set_xlabel('Iteration')
    ax.set_ylabel('Loss')
    ax.set_title('Convergence Comparison')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_yscale('log')

    plt.suptitle('–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä—ñ–≤ –Ω–∞ Rosenbrock Function', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()


compare_optimizers()

# ============================================================================
# –ß–ê–°–¢–ò–ù–ê 4: LEARNING RATE SCHEDULING
# ============================================================================

print("\n" + "=" * 60)
print("–ß–ê–°–¢–ò–ù–ê 4: LEARNING RATE SCHEDULING")
print("=" * 60)


def visualize_lr_schedules():
    """–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä—ñ–∑–Ω–∏—Ö —Å—Ç—Ä–∞—Ç–µ–≥—ñ–π –∑–º—ñ–Ω–∏ learning rate"""

    epochs = 100
    initial_lr = 0.1

    fig, axes = plt.subplots(2, 3, figsize=(15, 10))

    # 1. Constant
    lr_constant = [initial_lr] * epochs
    axes[0, 0].plot(lr_constant, 'b-', linewidth=2)
    axes[0, 0].set_title('Constant LR')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Learning Rate')
    axes[0, 0].grid(True, alpha=0.3)

    # 2. Step Decay
    lr_step = []
    lr = initial_lr
    for epoch in range(epochs):
        if epoch % 30 == 0 and epoch > 0:
            lr *= 0.5
        lr_step.append(lr)

    axes[0, 1].plot(lr_step, 'b-', linewidth=2)
    axes[0, 1].set_title('Step Decay (every 30 epochs)')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Learning Rate')
    axes[0, 1].grid(True, alpha=0.3)

    # 3. Exponential Decay
    lr_exp = [initial_lr * (0.95 ** epoch) for epoch in range(epochs)]
    axes[0, 2].plot(lr_exp, 'b-', linewidth=2)
    axes[0, 2].set_title('Exponential Decay')
    axes[0, 2].set_xlabel('Epoch')
    axes[0, 2].set_ylabel('Learning Rate')
    axes[0, 2].grid(True, alpha=0.3)

    # 4. Cosine Annealing
    lr_cosine = [initial_lr * (1 + np.cos(np.pi * epoch / epochs)) / 2
                 for epoch in range(epochs)]
    axes[1, 0].plot(lr_cosine, 'b-', linewidth=2)
    axes[1, 0].set_title('Cosine Annealing')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Learning Rate')
    axes[1, 0].grid(True, alpha=0.3)

    # 5. Warm-up + Decay
    lr_warmup = []
    warmup_epochs = 10
    for epoch in range(epochs):
        if epoch < warmup_epochs:
            lr = initial_lr * (epoch + 1) / warmup_epochs
        else:
            lr = initial_lr * (0.95 ** (epoch - warmup_epochs))
        lr_warmup.append(lr)

    axes[1, 1].plot(lr_warmup, 'b-', linewidth=2)
    axes[1, 1].axvline(x=warmup_epochs, color='r', linestyle='--', alpha=0.5)
    axes[1, 1].set_title('Warm-up + Exponential Decay')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Learning Rate')
    axes[1, 1].grid(True, alpha=0.3)

    # 6. Cyclic (Triangle)
    lr_cyclic = []
    cycle_length = 20
    for epoch in range(epochs):
        cycle = epoch % cycle_length
        if cycle < cycle_length / 2:
            lr = initial_lr * 0.1 + (initial_lr - initial_lr * 0.1) * cycle / (cycle_length / 2)
        else:
            lr = initial_lr - (initial_lr - initial_lr * 0.1) * (cycle - cycle_length / 2) / (cycle_length / 2)
        lr_cyclic.append(lr)

    axes[1, 2].plot(lr_cyclic, 'b-', linewidth=2)
    axes[1, 2].set_title('Cyclic LR')
    axes[1, 2].set_xlabel('Epoch')
    axes[1, 2].set_ylabel('Learning Rate')
    axes[1, 2].grid(True, alpha=0.3)

    plt.suptitle('–°—Ç—Ä–∞—Ç–µ–≥—ñ—ó –∑–º—ñ–Ω–∏ Learning Rate', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.show()


visualize_lr_schedules()


# Learning Rate Finder
def find_optimal_lr(model, dataloader, start_lr=1e-7, end_lr=10, num_iter=100):
    """
    –†–µ–∞–ª—ñ–∑–∞—Ü—ñ—è Learning Rate Finder
    """
    model.train()
    optimizer = optim.SGD(model.parameters(), lr=start_lr)

    lrs = []
    losses = []

    # –ï–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–π–Ω–µ –∑–±—ñ–ª—å—à–µ–Ω–Ω—è lr
    lr_mult = (end_lr / start_lr) ** (1 / num_iter)

    current_lr = start_lr
    best_loss = float('inf')

    for batch_idx, (data, target) in enumerate(dataloader):
        if batch_idx >= num_iter:
            break

        # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ lr
        for param_group in optimizer.param_groups:
            param_group['lr'] = current_lr

        optimizer.zero_grad()
        output = model(data)
        loss = F.cross_entropy(output, target)

        # –Ø–∫—â–æ –≤—Ç—Ä–∞—Ç–∏ –≤–∏–±—É—Ö–∞—é—Ç—å - –∑—É–ø–∏–Ω—è—î–º–æ
        if loss.item() > best_loss * 4:
            break

        if loss.item() < best_loss:
            best_loss = loss.item()

        losses.append(loss.item())
        lrs.append(current_lr)

        loss.backward()
        optimizer.step()

        current_lr *= lr_mult

    # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
    plt.figure(figsize=(10, 6))
    plt.plot(lrs, losses)
    plt.xscale('log')
    plt.xlabel('Learning Rate (log scale)')
    plt.ylabel('Loss')
    plt.title('Learning Rate Finder')
    plt.grid(True, alpha=0.3)

    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Ç–æ—á–∫—É –∑ –Ω–∞–π–±—ñ–ª—å—à–∏–º —Å–ø–∞–¥–æ–º
    gradients = np.gradient(losses)
    best_lr_idx = np.argmin(gradients)
    best_lr = lrs[best_lr_idx]

    plt.axvline(x=best_lr, color='r', linestyle='--', label=f'Suggested LR: {best_lr:.2e}')
    plt.legend()
    plt.show()

    return best_lr


print("\nüí° Learning Rate Finder –¥–æ–ø–æ–º–∞–≥–∞—î –∑–Ω–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∏–π –ø–æ—á–∞—Ç–∫–æ–≤–∏–π learning rate")
print("–®—É–∫–∞—î–º–æ —Ç–æ—á–∫—É, –¥–µ loss –ø–æ—á–∏–Ω–∞—î —à–≤–∏–¥–∫–æ –ø–∞–¥–∞—Ç–∏, –∞–ª–µ –¥–æ —Ç–æ–≥–æ —è–∫ –ø–æ—á–∏–Ω–∞—î —Ä–æ–∑—Ö–æ–¥–∏—Ç–∏—Å—è")

# ============================================================================
# –ß–ê–°–¢–ò–ù–ê 5: –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ê –ü–†–û–ë–õ–ï–ú –ù–ê–í–ß–ê–ù–ù–Ø
# ============================================================================

print("\n" + "=" * 60)
print("–ß–ê–°–¢–ò–ù–ê 5: –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ê –ü–†–û–ë–õ–ï–ú")
print("=" * 60)


class TrainingDiagnostics:
    """–ö–ª–∞—Å –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Ç–∏–ø–æ–≤–∏—Ö –ø—Ä–æ–±–ª–µ–º –Ω–∞–≤—á–∞–Ω–Ω—è"""

    def __init__(self):
        self.history = {
            'train_loss': [],
            'val_loss': [],
            'gradients': [],
            'weights': []
        }

    def diagnose_vanishing_gradient(self, gradients):
        """–î–µ—Ç–µ–∫—Ç—É—î vanishing gradient problem"""
        grad_norms = [torch.norm(g).item() for g in gradients]
        avg_norm = np.mean(grad_norms)

        if avg_norm < 1e-5:
            return "‚ö†Ô∏è VANISHING GRADIENT: –ì—Ä–∞–¥—ñ—î–Ω—Ç–∏ –∑–∞–Ω–∞–¥—Ç–æ –º–∞–ª—ñ!"
        elif avg_norm < 1e-3:
            return "‚ö° –ü–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è: –ì—Ä–∞–¥—ñ—î–Ω—Ç–∏ –º–∞–ª—ñ, –º–æ–∂–ª–∏–≤—ñ –ø—Ä–æ–±–ª–µ–º–∏"
        else:
            return "‚úÖ –ì—Ä–∞–¥—ñ—î–Ω—Ç–∏ –≤ –Ω–æ—Ä–º—ñ"

    def diagnose_exploding_gradient(self, gradients):
        """–î–µ—Ç–µ–∫—Ç—É—î exploding gradient problem"""
        grad_norms = [torch.norm(g).item() for g in gradients]
        max_norm = np.max(grad_norms)

        if max_norm > 100:
            return "üî• EXPLODING GRADIENT: –ì—Ä–∞–¥—ñ—î–Ω—Ç–∏ –≤–∏–±—É—Ö–∞—é—Ç—å!"
        elif max_norm > 10:
            return "‚ö° –ü–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è: –í–µ–ª–∏–∫—ñ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏"
        else:
            return "‚úÖ –ì—Ä–∞–¥—ñ—î–Ω—Ç–∏ —Å—Ç–∞–±—ñ–ª—å–Ω—ñ"

    def diagnose_overfitting(self, train_loss, val_loss, threshold=0.1):
        """–î–µ—Ç–µ–∫—Ç—É—î overfitting"""
        if len(train_loss) < 10 or len(val_loss) < 10:
            return "–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"

        recent_train = np.mean(train_loss[-10:])
        recent_val = np.mean(val_loss[-10:])

        if recent_val > recent_train * (1 + threshold):
            gap = (recent_val - recent_train) / recent_train * 100
            return f"üìà OVERFITTING: Val loss –Ω–∞ {gap:.1f}% –≤–∏—â–µ train loss!"
        else:
            return "‚úÖ –ù–µ–º–∞—î –æ–∑–Ω–∞–∫ overfitting"

    def diagnose_underfitting(self, train_loss, target_loss):
        """–î–µ—Ç–µ–∫—Ç—É—î underfitting"""
        if len(train_loss) < 10:
            return "–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"

        recent_train = np.mean(train_loss[-10:])

        if recent_train > target_loss * 1.5:
            return f"üìâ UNDERFITTING: Train loss –≤—Å–µ —â–µ –≤–∏—Å–æ–∫–∏–π ({recent_train:.4f})!"
        else:
            return "‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞–≤—á–∞—î—Ç—å—Å—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ"

    def visualize_diagnostics(self, model_name="Model"):
        """–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –Ω–∞–≤—á–∞–Ω–Ω—è"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))

        # 1. Loss curves
        axes[0, 0].plot(self.history['train_loss'], 'b-', label='Train')
        axes[0, 0].plot(self.history['val_loss'], 'r-', label='Validation')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].set_title('Training vs Validation Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # 2. Gradient norms
        if self.history['gradients']:
            grad_norms = [np.mean([torch.norm(g).item() for g in grads])
                          for grads in self.history['gradients']]
            axes[0, 1].plot(grad_norms, 'g-')
            axes[0, 1].set_xlabel('Iteration')
            axes[0, 1].set_ylabel('Gradient Norm')
            axes[0, 1].set_title('Gradient Norm Evolution')
            axes[0, 1].set_yscale('log')
            axes[0, 1].grid(True, alpha=0.3)

        # 3. Weight distribution
        if self.history['weights']:
            latest_weights = self.history['weights'][-1]
            all_weights = torch.cat([w.flatten() for w in latest_weights])
            axes[1, 0].hist(all_weights.detach().cpu().numpy(), bins=50, alpha=0.7, color='purple')
            axes[1, 0].set_xlabel('Weight Value')
            axes[1, 0].set_ylabel('Count')
            axes[1, 0].set_title('Weight Distribution')
            axes[1, 0].grid(True, alpha=0.3)

        # 4. Learning dynamics
        if len(self.history['train_loss']) > 1:
            loss_changes = np.diff(self.history['train_loss'])
            axes[1, 1].plot(loss_changes, 'orange')
            axes[1, 1].axhline(y=0, color='black', linestyle='-', alpha=0.3)
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('Loss Change')
            axes[1, 1].set_title('Loss Change per Epoch')
            axes[1, 1].grid(True, alpha=0.3)

        plt.suptitle(f'Training Diagnostics: {model_name}', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.show()


# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
def demonstrate_training_problems():
    """–°–∏–º—É–ª—è—Ü—ñ—è —Ç–∞ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ä—ñ–∑–Ω–∏—Ö –ø—Ä–æ–±–ª–µ–º –Ω–∞–≤—á–∞–Ω–Ω—è"""

    print("\nüîç –î–ï–ú–û–ù–°–¢–†–ê–¶–Ü–Ø –¢–ò–ü–û–í–ò–• –ü–†–û–ë–õ–ï–ú –ù–ê–í–ß–ê–ù–ù–Ø\n")

    diagnostics = TrainingDiagnostics()

    # 1. Vanishing Gradient
    print("1. Vanishing Gradient Problem:")
    small_gradients = [torch.randn(10, 10) * 1e-6 for _ in range(5)]
    print(f"   {diagnostics.diagnose_vanishing_gradient(small_gradients)}")

    # 2. Exploding Gradient
    print("\n2. Exploding Gradient Problem:")
    large_gradients = [torch.randn(10, 10) * 1000 for _ in range(5)]
    print(f"   {diagnostics.diagnose_exploding_gradient(large_gradients)}")

    # 3. Overfitting
    print("\n3. Overfitting Detection:")
    train_loss = [0.5, 0.3, 0.2, 0.15, 0.1, 0.08, 0.06, 0.04, 0.03, 0.02]
    val_loss = [0.5, 0.35, 0.3, 0.28, 0.27, 0.28, 0.30, 0.35, 0.40, 0.45]
    print(f"   {diagnostics.diagnose_overfitting(train_loss, val_loss)}")

    # 4. Underfitting
    print("\n4. Underfitting Detection:")
    high_train_loss = [0.8, 0.75, 0.72, 0.70, 0.68, 0.67, 0.66, 0.65, 0.64, 0.63]
    print(f"   {diagnostics.diagnose_underfitting(high_train_loss, target_loss=0.1)}")


demonstrate_training_problems()

# ============================================================================
# –ß–ê–°–¢–ò–ù–ê 6: GRADIENT CLIPPING
# ============================================================================

print("\n" + "=" * 60)
print("–ß–ê–°–¢–ò–ù–ê 6: GRADIENT CLIPPING")
print("=" * 60)


def demonstrate_gradient_clipping():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—è gradient clipping"""

    # –°—Ç–≤–æ—Ä—é—î–º–æ –º–æ–¥–µ–ª—å –∑ –≤–µ–ª–∏–∫–∏–º–∏ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞–º–∏
    model = nn.Sequential(
        nn.Linear(10, 50),
        nn.ReLU(),
        nn.Linear(50, 50),
        nn.ReLU(),
        nn.Linear(50, 1)
    )

    # –°–∏–º—É–ª—é—î–º–æ –≤–µ–ª–∏–∫—ñ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏
    for param in model.parameters():
        param.grad = torch.randn_like(param) * 100  # –í–µ–ª–∏–∫—ñ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏!

    print("üìä –ì—Ä–∞–¥—ñ—î–Ω—Ç–∏ –î–û clipping:")
    total_norm_before = 0
    for param in model.parameters():
        param_norm = param.grad.norm(2)
        total_norm_before += param_norm.item() ** 2
    total_norm_before = total_norm_before ** 0.5
    print(f"   Total gradient norm: {total_norm_before:.2f}")

    # Gradient clipping
    max_norm = 1.0
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)

    print(f"\n‚úÇÔ∏è –ó–∞—Å—Ç–æ—Å–æ–≤—É—î–º–æ gradient clipping (max_norm={max_norm})")

    print("\nüìä –ì—Ä–∞–¥—ñ—î–Ω—Ç–∏ –ü–Ü–°–õ–Ø clipping:")
    total_norm_after = 0
    for param in model.parameters():
        param_norm = param.grad.norm(2)
        total_norm_after += param_norm.item() ** 2
    total_norm_after = total_norm_after ** 0.5
    print(f"   Total gradient norm: {total_norm_after:.2f}")

    print(f"\n‚úÖ –ó–º–µ–Ω—à–µ–Ω–Ω—è –Ω–æ—Ä–º–∏: {total_norm_before / total_norm_after:.2f}x")


demonstrate_gradient_clipping()

# ============================================================================
# –ß–ê–°–¢–ò–ù–ê 7: –ü–û–í–ù–ò–ô –¢–†–ï–ù–£–í–ê–õ–¨–ù–ò–ô PIPELINE
# ============================================================================

print("\n" + "=" * 60)
print("–ß–ê–°–¢–ò–ù–ê 7: –ü–û–í–ù–ò–ô –¢–†–ï–ù–£–í–ê–õ–¨–ù–ò–ô PIPELINE")
print("=" * 60)


class CompletePipeline:
    """–ü–æ–≤–Ω–∏–π pipeline –Ω–∞–≤—á–∞–Ω–Ω—è –∑ —É—Å—ñ–º–∞ best practices"""

    def __init__(self, model, device='cpu'):
        self.model = model.to(device)
        self.device = device
        self.diagnostics = TrainingDiagnostics()

    def train_epoch(self, dataloader, optimizer, criterion, clip_grad=None):
        """–û–¥–Ω–∞ –µ–ø–æ—Ö–∞ –Ω–∞–≤—á–∞–Ω–Ω—è"""
        self.model.train()
        total_loss = 0

        for batch_idx, (data, target) in enumerate(dataloader):
            data, target = data.to(self.device), target.to(self.device)

            # Forward pass
            optimizer.zero_grad()
            output = self.model(data)
            loss = criterion(output, target)

            # Backward pass
            loss.backward()

            # Gradient clipping —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
            if clip_grad is not None:
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip_grad)

            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            if batch_idx == 0:  # –¢—ñ–ª—å–∫–∏ –ø–µ—Ä—à–∏–π –±–∞—Ç—á –∫–æ–∂–Ω–æ—ó –µ–ø–æ—Ö–∏
                grads = [param.grad.clone() for param in self.model.parameters() if param.grad is not None]
                self.diagnostics.history['gradients'].append(grads)

            # Update
            optimizer.step()

            total_loss += loss.item()

        return total_loss / len(dataloader)

    def validate(self, dataloader, criterion):
        """–í–∞–ª—ñ–¥–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ"""
        self.model.eval()
        total_loss = 0

        with torch.no_grad():
            for data, target in dataloader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                loss = criterion(output, target)
                total_loss += loss.item()

        return total_loss / len(dataloader)

    def train(self, train_loader, val_loader, epochs=10, lr=0.001,
              optimizer_name='Adam', scheduler_type='StepLR'):
        """–ü–æ–≤–Ω–∏–π —Ü–∏–∫–ª –Ω–∞–≤—á–∞–Ω–Ω—è"""

        # –í–∏–±—ñ—Ä –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∞
        if optimizer_name == 'Adam':
            optimizer = optim.Adam(self.model.parameters(), lr=lr)
        elif optimizer_name == 'SGD':
            optimizer = optim.SGD(self.model.parameters(), lr=lr, momentum=0.9)
        else:
            optimizer = optim.RMSprop(self.model.parameters(), lr=lr)

        # Learning rate scheduler
        if scheduler_type == 'StepLR':
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
        elif scheduler_type == 'CosineAnnealingLR':
            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        else:
            scheduler = None

        criterion = nn.CrossEntropyLoss()

        print(f"üöÄ –ù–∞–≤—á–∞–Ω–Ω—è –∑ {optimizer_name} –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–æ–º —Ç–∞ {scheduler_type} scheduler")
        print("=" * 50)

        for epoch in range(epochs):
            # Training
            train_loss = self.train_epoch(train_loader, optimizer, criterion, clip_grad=1.0)
            self.diagnostics.history['train_loss'].append(train_loss)

            # Validation
            val_loss = self.validate(val_loader, criterion)
            self.diagnostics.history['val_loss'].append(val_loss)

            # Learning rate scheduling
            if scheduler is not None:
                scheduler.step()
                current_lr = scheduler.get_last_lr()[0]
            else:
                current_lr = lr

            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –≤–∞–≥–∏ –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            weights = [param.clone() for param in self.model.parameters()]
            self.diagnostics.history['weights'].append(weights)

            # –í–∏–≤–µ–¥–µ–Ω–Ω—è –ø—Ä–æ–≥—Ä–µ—Å—É
            if (epoch + 1) % 5 == 0:
                print(f"Epoch [{epoch + 1}/{epochs}] "
                      f"Train Loss: {train_loss:.4f}, "
                      f"Val Loss: {val_loss:.4f}, "
                      f"LR: {current_lr:.6f}")

                # –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
                if len(self.diagnostics.history['gradients']) > 0:
                    grad_check = self.diagnostics.diagnose_exploding_gradient(
                        self.diagnostics.history['gradients'][-1]
                    )
                    if "EXPLODING" in grad_check or "–ü–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è" in grad_check:
                        print(f"   {grad_check}")

        print("\nüìä –§—ñ–Ω–∞–ª—å–Ω–∞ –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:")
        print(f"   {self.diagnostics.diagnose_overfitting(self.diagnostics.history['train_loss'],
                                                          self.diagnostics.history['val_loss'])}")

        # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è
        self.diagnostics.visualize_diagnostics(f"{optimizer_name} + {scheduler_type}")

        return self.diagnostics.history


# –¢–µ—Å—Ç pipeline –Ω–∞ –ø—Ä–æ—Å—Ç—ñ–π –∑–∞–¥–∞—á—ñ
def _complete_pipeline_test():
    """–¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è –ø–æ–≤–Ω–æ–≥–æ pipeline"""

    # –ì–µ–Ω–µ—Ä—É—î–º–æ –¥–∞–Ω—ñ
    from sklearn.datasets import make_classification
    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,
                               n_redundant=5, n_classes=3, random_state=42)

    X_train, X_val = X[:800], X[800:]
    y_train, y_val = y[:800], y[800:]

    # –°—Ç–≤–æ—Ä—é—î–º–æ DataLoaders
    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))
    val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.LongTensor(y_val))

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

    # –°—Ç–≤–æ—Ä—é—î–º–æ –º–æ–¥–µ–ª—å
    model = nn.Sequential(
        nn.Linear(20, 64),
        nn.ReLU(),
        nn.Dropout(0.2),
        nn.Linear(64, 32),
        nn.ReLU(),
        nn.Dropout(0.2),
        nn.Linear(32, 3)
    )

    # –ù–∞–≤—á–∞–Ω–Ω—è
    pipeline = CompletePipeline(model, device=device)
    history = pipeline.train(train_loader, val_loader, epochs=30,
                             lr=0.01, optimizer_name='Adam',
                             scheduler_type='CosineAnnealingLR')


print("\nüß™ –¢–µ—Å—Ç—É—î–º–æ –ø–æ–≤–Ω–∏–π pipeline –Ω–∞ —Å–∏–Ω—Ç–µ—Ç–∏—á–Ω–∏—Ö –¥–∞–Ω–∏—Ö:\n")
_complete_pipeline_test()

# ============================================================================
# –ü–Ü–î–°–£–ú–ö–ò
# ============================================================================

print("\n" + "=" * 60)
print("–ü–Ü–î–°–£–ú–ö–ò")
print("=" * 60)

print("""
‚úÖ –©–æ –º–∏ –≤–∏–≤—á–∏–ª–∏:

1. **Backpropagation**
   - –ü–æ–∫—Ä–æ–∫–æ–≤–µ –æ–±—á–∏—Å–ª–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤
   - Chain rule –≤ –¥—ñ—ó
   - –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø—Ä–æ–º—ñ–∂–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å

2. **–§—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç**
   - MSE, MAE, Huber –¥–ª—è —Ä–µ–≥—Ä–µ—Å—ñ—ó
   - BCE, Cross-Entropy –¥–ª—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
   - Focal Loss –¥–ª—è –Ω–µ–∑–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∏—Ö –¥–∞–Ω–∏—Ö

3. **–û–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∏**
   - SGD, Momentum, RMSprop, Adam
   - –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç—Ä–∞—î–∫—Ç–æ—Ä—ñ–π –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
   - –í–∏–±—ñ—Ä –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä–∞ –¥–ª—è –∑–∞–¥–∞—á—ñ

4. **Learning Rate**
   - –†—ñ–∑–Ω—ñ —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó scheduling
   - Learning Rate Finder
   - Warm-up —Å—Ç—Ä–∞—Ç–µ–≥—ñ—ó

5. **–î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º**
   - Vanishing/Exploding gradients
   - Overfitting/Underfitting
   - Gradient clipping

6. **Best Practices**
   - –ü–æ–≤–Ω–∏–π —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏–π pipeline
   - –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ –º–µ—Ç—Ä–∏–∫
   - –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –ø—Ä–æ—Ü–µ—Å—É –Ω–∞–≤—á–∞–Ω–Ω—è

üéØ –ö–ª—é—á–æ–≤—ñ –≤–∏—Å–Ω–æ–≤–∫–∏:
- Adam - —Ö–æ—Ä–æ—à–∏–π –≤–∏–±—ñ—Ä –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º
- Learning rate - –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏–π –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä
- Gradient clipping –≤–∞–∂–ª–∏–≤–∏–π –¥–ª—è RNN
- –ó–∞–≤–∂–¥–∏ —Å–ª—ñ–¥–∫—É–π—Ç–µ –∑–∞ train vs val loss
- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ scheduler –¥–ª—è –∫—Ä–∞—â–æ—ó –∑–±—ñ–∂–Ω–æ—Å—Ç—ñ

üöÄ –ù–∞—Å—Ç—É–ø–Ω—ñ –∫—Ä–æ–∫–∏:
- –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—è (Dropout, L2, BatchNorm)
- Data Augmentation
- Transfer Learning
- Hyperparameter Tuning
""")
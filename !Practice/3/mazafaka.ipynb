{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b0038ead8befa5",
   "metadata": {},
   "source": [
    "Частина 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431566ba3ddeadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.358801\n",
      "Manual gradients:\n",
      "dL_dW2:\n",
      " [[-0.08386]\n",
      " [-0.1198 ]\n",
      " [ 0.     ]]\n",
      "dL_dW1:\n",
      " [[-0.1797   0.1198   0.     ]\n",
      " [ 0.07188 -0.04792  0.     ]]\n",
      "\n",
      "PyTorch autograd gradients:\n",
      "W2 grad:\n",
      " tensor([[-0.0839],\n",
      "        [-0.1198],\n",
      "        [ 0.0000]])\n",
      "W1 grad:\n",
      " tensor([[-0.1797,  0.1198,  0.0000],\n",
      "        [ 0.0719, -0.0479, -0.0000]])\n",
      "W1 difference:\n",
      " [[ 2.07424164e-09  6.06775284e-09  0.00000000e+00]\n",
      " [-5.30004501e-09 -1.91926960e-10  0.00000000e+00]]\n",
      "\n",
      "Comparison (manual vs PyTorch):\n",
      "W2 difference:\n",
      " [[-4.99248504e-09]\n",
      " [-6.06775284e-09]\n",
      " [ 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def relu(x):    \n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    " \n",
    "def manual_backprop():\n",
    "    \"\"\"\n",
    "    Для простої мережі з одним прихованим шаром:\n",
    "    Input (2) → Hidden (3, ReLU) → Output (1, Linear)\n",
    "\n",
    "    Дано:\n",
    "    - X = [[0.5, -0.2]]  # вхід\n",
    "    - y_true = 0.7        # ціль\n",
    "    - W1 = [[0.1, 0.2, -0.3],\n",
    "            [0.4, -0.5, 0.6]]\n",
    "    - b1 = [0.1, -0.1, 0.2]\n",
    "    - W2 = [[0.3], [-0.2], [0.5]]\n",
    "    - b2 = [0.1]\n",
    "\n",
    "    Завдання:\n",
    "    1. Виконайте forward pass\n",
    "    2. Обчисліть MSE loss\n",
    "    3. Обчисліть всі градієнти вручну\n",
    "    4. Оновіть ваги з learning_rate = 0.1\n",
    "    5. Порівняйте з PyTorch autograd\n",
    "    \"\"\"\n",
    "    X = np.array([[0.5, -0.2]])\n",
    "    y_true = 0.7\n",
    "    W1 = np.array([[0.1, 0.2, -0.3],\n",
    "                   [0.4, -0.5, 0.6]])\n",
    "    b1 = np.array([0.1, -0.1, 0.2])\n",
    "    W2 = np.array([[0.3], [-0.2], [0.5]])\n",
    "    b2 = np.array([0.1])\n",
    "\n",
    "     \n",
    "    # Forward pass\n",
    "    z1 = X @ W1 + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = a1 @ W2 + b2\n",
    "    y_pred = z2 \n",
    "\n",
    "    #loss \n",
    "    loss = np.mean((y_pred - y_true) ** 2)    \n",
    "    print(f\"Loss: {loss}\")\n",
    "\n",
    "    #backward pass  обчислюються похідні градієнти       \n",
    "\n",
    "    # --- BACKWARD PASS ---\n",
    "    # Похідна функції втрат \n",
    "    dL_dy_pred = 2 * (y_pred - y_true) # (1, 1) \n",
    "\n",
    "    # похідна втрат другого шару \n",
    "    dL_dW2 = a1.T @ dL_dy_pred # (3, 1)\n",
    "    dL_db2 = dL_dy_pred # (1, 1)\n",
    "\n",
    "    # як зміна виходу прихованого шару впливає на втрати\n",
    "    dL_da1 = dL_dy_pred @ W2.T                 # (1, 3)\n",
    "    dL_dz1 = dL_da1 * relu_derivative(z1)      # (1, 3)\n",
    "\n",
    "    # градієнт першого шару \n",
    "    dL_dW1 = X.T @ dL_dz1                      # (2, 3)\n",
    "    # бо зсув додається до виходу \n",
    "    dL_db1 = dL_dz1                            # (1, 3)\n",
    "\n",
    "    # оновлення ваг за правилом градієнтного спуску\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    W1 -= learning_rate * dL_dW1\n",
    "    b1 -= learning_rate * dL_db1.squeeze()\n",
    "    W2 -= learning_rate * dL_dW2\n",
    "    b2 -= learning_rate * dL_db2.squeeze()\n",
    "\n",
    "    print(\"Manual gradients:\")\n",
    "    print(\"dL_dW2:\\n\", dL_dW2)\n",
    "    print(\"dL_dW1:\\n\", dL_dW1)\n",
    "\n",
    "    # PYTORCH CHECK перевірка правильності моїх обчислень\n",
    "    X_torch = torch.tensor([[0.5, -0.2]], dtype=torch.float32)\n",
    "    y_torch = torch.tensor([[0.7]], dtype=torch.float32)\n",
    "\n",
    "    W1_torch = torch.tensor([[0.1, 0.2, -0.3],\n",
    "                             [0.4, -0.5, 0.6]], dtype=torch.float32, requires_grad=True)\n",
    "    b1_torch = torch.tensor([0.1, -0.1, 0.2], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    W2_torch = torch.tensor([[0.3], [-0.2], [0.5]], dtype=torch.float32, requires_grad=True)\n",
    "    b2_torch = torch.tensor([0.1], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    # Forward\n",
    "    z1_torch = X_torch @ W1_torch + b1_torch\n",
    "    a1_torch = torch.relu(z1_torch)\n",
    "    y_pred_torch = a1_torch @ W2_torch + b2_torch\n",
    "\n",
    "    loss_torch = torch.mean((y_pred_torch - y_torch) ** 2)\n",
    "    loss_torch.backward()\n",
    "\n",
    "    print(\"\\nPyTorch autograd gradients:\")\n",
    "    print(\"W2 grad:\\n\", W2_torch.grad)\n",
    "    print(\"W1 grad:\\n\", W1_torch.grad)\n",
    "\n",
    "    # Сравнение\n",
    "    print(\"W1 difference:\\n\", dL_dW1 - W1_torch.grad.detach().numpy())\n",
    "    print(\"\\nComparison (manual vs PyTorch):\")\n",
    "    print(\"W2 difference:\\n\", dL_dW2 - W2_torch.grad.detach().numpy())\n",
    "\n",
    "\n",
    "manual_backprop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7911ca713be990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE  : 0.04666666666666667 [-0.2         0.13333333 -0.06666667]\n",
      "MAE  : 0.19999999999999998 [-0.33333333  0.33333333 -0.33333333]\n",
      "Huber: 0.023333333333333334 [-0.1         0.06666667 -0.03333333]\n",
      "BCE  : 0.22839300363692283 [-0.47619048  0.41666667 -0.37037037]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LossFunctions:\n",
    "    \"\"\"\n",
    "    Реалізуйте наступні функції втрат та їх градієнти:\n",
    "    1. MSE (Mean Squared Error)\n",
    "    2. MAE (Mean Absolute Error)\n",
    "    3. Huber Loss (комбінація MSE та MAE)\n",
    "    4. Binary Cross-Entropy\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(y_pred, y_true): # середньоквадратична помилка\n",
    "        \"\"\"MSE loss та її градієнт\"\"\"\n",
    "        loss = np.mean((y_pred - y_true) ** 2)\n",
    "        gradient = (2 / y_true.size) * (y_pred - y_true)\n",
    "        return loss, gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def mae(y_pred, y_true): # середньоабсолютна помилка\n",
    "        \"\"\"MAE loss та її градієнт\"\"\"\n",
    "        loss = np.mean(np.abs(y_pred - y_true))\n",
    "        # градієнт не визначений в 0 тому sign()\n",
    "        gradient = np.sign(y_pred - y_true) / y_true.size\n",
    "        return loss, gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def huber(y_pred, y_true, delta=1.0): # комінація MSE та MAE\n",
    "        \"\"\"\n",
    "        Huber loss:\n",
    "        L = 0.5 * (y_pred - y_true)^2,           якщо |y_pred - y_true| <= delta\n",
    "        L = delta * |y_pred - y_true| - 0.5 * delta^2,  інакше\n",
    "        \"\"\"\n",
    "        error = y_pred - y_true\n",
    "        abs_error = np.abs(error) # не може бути відємним\n",
    "\n",
    "        is_small_error = abs_error <= delta\n",
    "\n",
    "        # втрати\n",
    "        loss = np.where(is_small_error,\n",
    "                        0.5 * error ** 2,\n",
    "                        delta * abs_error - 0.5 * delta ** 2)\n",
    "        loss = np.mean(loss)\n",
    "\n",
    "        # граднієнт\n",
    "        gradient = np.where(is_small_error,\n",
    "                            error,\n",
    "                            delta * np.sign(error))\n",
    "        gradient = gradient / y_true.size\n",
    "        return loss, gradient\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def binary_cross_entropy(y_pred, y_true): # все використовується для бінарної класифікації 0 або 1\n",
    "        \"\"\"BCE loss з численною стабільністю\"\"\"\n",
    "        eps = 1e-12\n",
    "        # np.clip щоб уникнути log(0)\n",
    "        y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        gradient = (-(y_true / y_pred) + ((1 - y_true) / (1 - y_pred))) / y_true.size\n",
    "        return loss, gradient\n",
    "\n",
    "\n",
    "# === Тест ===\n",
    "losses = LossFunctions()\n",
    "y_pred = np.array([0.7, 0.2, 0.9])\n",
    "y_true = np.array([1.0, 0.0, 1.0])\n",
    "\n",
    "mse_loss, mse_grad = losses.mse(y_pred, y_true)\n",
    "mae_loss, mae_grad = losses.mae(y_pred, y_true)\n",
    "huber_loss, huber_grad = losses.huber(y_pred, y_true, delta=0.5)\n",
    "bce_loss, bce_grad = losses.binary_cross_entropy(y_pred, y_true)\n",
    "\n",
    "print(\"MSE  :\", mse_loss, mse_grad)\n",
    "print(\"MAE  :\", mae_loss, mae_grad)\n",
    "print(\"Huber:\", huber_loss, huber_grad)\n",
    "print(\"BCE  :\", bce_loss, bce_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3281faf5740f4926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SGD:\n",
      "Step  1: w = 9.0000, loss = 16.0000\n",
      "Step  2: w = 8.2000, loss = 10.2400\n",
      "Step  3: w = 7.5600, loss = 6.5536\n",
      "Step  4: w = 7.0480, loss = 4.1943\n",
      "Step  5: w = 6.6384, loss = 2.6844\n",
      "Step  6: w = 6.3107, loss = 1.7180\n",
      "Step  7: w = 6.0486, loss = 1.0995\n",
      "Step  8: w = 5.8389, loss = 0.7037\n",
      "Step  9: w = 5.6711, loss = 0.4504\n",
      "Step 10: w = 5.5369, loss = 0.2882\n",
      "\n",
      "SGD+Momentum:\n",
      "Step  1: w = 9.0000, loss = 16.0000\n",
      "Step  2: w = 7.3000, loss = 5.2900\n",
      "Step  3: w = 5.3100, loss = 0.0961\n",
      "Step  4: w = 3.4570, loss = 2.3808\n",
      "Step  5: w = 2.0979, loss = 8.4222\n",
      "Step  6: w = 1.4551, loss = 12.5661\n",
      "Step  7: w = 1.5856, loss = 11.6581\n",
      "Step  8: w = 2.3859, loss = 6.8334\n",
      "Step  9: w = 3.6290, loss = 1.8796\n",
      "Step 10: w = 5.0220, loss = 0.0005\n",
      "\n",
      "Adam:\n",
      "Step  1: w = 9.9000, loss = 24.0100\n",
      "Step  2: w = 9.8001, loss = 23.0406\n",
      "Step  3: w = 9.7002, loss = 22.0920\n",
      "Step  4: w = 9.6005, loss = 21.1647\n",
      "Step  5: w = 9.5010, loss = 20.2588\n",
      "Step  6: w = 9.4017, loss = 19.3748\n",
      "Step  7: w = 9.3026, loss = 18.5128\n",
      "Step  8: w = 9.2039, loss = 17.6730\n",
      "Step  9: w = 9.1056, loss = 16.8556\n",
      "Step 10: w = 9.0076, loss = 16.0609\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CustomOptimizer:\n",
    "    \"\"\"Базовий клас для оптимізаторів\"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=0.01):\n",
    "        self.params = params  # список параметрів з .data і .grad)\n",
    "        self.lr = lr\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"Обнулення градієнтів перед новим кроком\"\"\"\n",
    "        for param in self.params:\n",
    "            param.grad = np.zeros_like(param.data)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Оновлення параметрів (абстрактний метод)\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "# SGD \n",
    "class MySGD(CustomOptimizer):\n",
    "    \"\"\"Stochastic Gradient Descent\"\"\"\n",
    "    def step(self): # приостий градієнтний спуск \n",
    "        for param in self.params:\n",
    "            param.data -= self.lr * param.grad\n",
    "\n",
    "\n",
    "# SGD + Momentum \n",
    "class MySGDMomentum(CustomOptimizer):\n",
    "    \"\"\"SGD з Momentum\"\"\"\n",
    "    def __init__(self, params, lr=0.01, momentum=0.9):\n",
    "        super().__init__(params, lr) # конструктор батьківського класу можна використовувати \n",
    "        self.momentum = momentum # додаємо моментум тіпа рух\n",
    "        # Ініціалізація velocity для кожного параметра\n",
    "        self.velocity = [np.zeros_like(p.data) for p in self.params]\n",
    "\n",
    "    def step(self):\n",
    "        for i, param in enumerate(self.params):\n",
    "            self.velocity[i] = self.momentum * self.velocity[i] - self.lr * param.grad\n",
    "            param.data += self.velocity[i]\n",
    "\n",
    "\n",
    "# Adam \n",
    "class MyAdam(CustomOptimizer):\n",
    "    \"\"\"Спрощена версія Adam\"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        super().__init__(params, lr)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        # Ініціалізація моментів\n",
    "        self.m = [np.zeros_like(p.data) for p in self.params]\n",
    "        self.v = [np.zeros_like(p.data) for p in self.params]\n",
    "        self.t = 0  # лічильник кроків (для bias correction)\n",
    "\n",
    "    def step(self):\n",
    "        self.t += 1\n",
    "        for i, param in enumerate(self.params):\n",
    "            g = param.grad\n",
    "\n",
    "            # Update biased first moment estimate\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * g\n",
    "            # Update biased second raw moment estimate\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (g ** 2)\n",
    "\n",
    "            # Bias correction\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # Update parameter\n",
    "            param.data -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "\n",
    "\n",
    "class Tensor:\n",
    "    \"\"\"Проста обгортка для зберігання значення і градієнта\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data, dtype=float) # значення параметру \n",
    "        self.grad = np.zeros_like(self.data) # градієнт параметру\n",
    "\n",
    "# Тестова функція: f(w) = (w - 5)^2\n",
    "def loss_fn(param):\n",
    "    return np.mean((param.data - 5) ** 2)\n",
    "\n",
    "def compute_grad(param):\n",
    "    param.grad = 2 * (param.data - 5)\n",
    "\n",
    "\n",
    "# Тест \n",
    "w = Tensor([10.0]) # 10.0\n",
    "params = [w] # 0\n",
    "\n",
    "opt_sgd = MySGD(params, lr=0.1)\n",
    "opt_momentum = MySGDMomentum([Tensor([10.0])], lr=0.1, momentum=0.9)\n",
    "opt_adam = MyAdam([Tensor([10.0])], lr=0.1)\n",
    "\n",
    "for opt, name in [(opt_sgd, \"SGD\"), (opt_momentum, \"SGD+Momentum\"), (opt_adam, \"Adam\")]:\n",
    "    w = Tensor([10.0])\n",
    "    opt.params = [w]\n",
    "    print(f\"\\n{name}:\")\n",
    "    for step in range(10):\n",
    "        compute_grad(w)\n",
    "        opt.step()\n",
    "        print(f\"Step {step+1:2d}: w = {w.data[0]:.4f}, loss = {loss_fn(w):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917947c945bab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LRScheduler:\n",
    "    \"\"\"Базовий клас для LR scheduler\"\"\"\n",
    "    def __init__(self, optimizer, initial_lr):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "        self.current_lr = initial_lr\n",
    "        self.epoch = 0\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"Оновити learning rate\"\"\"\n",
    "        self.epoch += 1\n",
    "        new_lr = self.compute_lr()\n",
    "        self.update_optimizer_lr(new_lr)\n",
    "        return new_lr\n",
    "\n",
    "    def compute_lr(self):\n",
    "        \"\"\"Обчислити новий LR\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update_optimizer_lr(self, new_lr):\n",
    "        \"\"\"Оновити LR в оптимізаторі\"\"\"\n",
    "        self.current_lr = new_lr\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = new_lr\n",
    "\n",
    "\n",
    "#  StepLR\n",
    "class StepLRScheduler(LRScheduler):\n",
    "    \"\"\"Зменшує LR кожні step_size епох\"\"\"\n",
    "    def __init__(self, optimizer, initial_lr, step_size=30, gamma=0.1):\n",
    "        super().__init__(optimizer, initial_lr)\n",
    "        self.step_size = step_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def compute_lr(self):\n",
    "        # lr = initial_lr * (gamma ^ floor(epoch / step_size))\n",
    "        return self.initial_lr * (self.gamma ** (self.epoch // self.step_size))\n",
    "\n",
    "\n",
    "# ExponentialLR\n",
    "class ExponentialLRScheduler(LRScheduler):\n",
    "    \"\"\"Експоненційне зменшення LR\"\"\"\n",
    "    def __init__(self, optimizer, initial_lr, decay_rate=0.95):\n",
    "        super().__init__(optimizer, initial_lr)\n",
    "        self.decay_rate = decay_rate\n",
    "\n",
    "    def compute_lr(self):\n",
    "        # lr = initial_lr * decay_rate^epoch\n",
    "        return self.initial_lr * (self.decay_rate ** self.epoch)\n",
    "\n",
    "\n",
    "# ===== CosineAnnealingLR =====\n",
    "class CosineAnnealingScheduler(LRScheduler):\n",
    "    \"\"\"Косинусне зменшення LR\"\"\"\n",
    "    def __init__(self, optimizer, initial_lr, T_max):\n",
    "        super().__init__(optimizer, initial_lr)\n",
    "        self.T_max = T_max\n",
    "\n",
    "    def compute_lr(self):\n",
    "        # lr = initial_lr * (1 + cos(pi * epoch / T_max)) / 2\n",
    "        return self.initial_lr * (1 + np.cos(np.pi * self.epoch / self.T_max)) / 2\n",
    "\n",
    "# Фейковий оптимізатор (імітує PyTorch-стиль)\n",
    "class DummyOptimizer:\n",
    "    def __init__(self, lr):\n",
    "        self.param_groups = [{'lr': lr}]\n",
    "\n",
    "# Ініціалізація\n",
    "optimizer = DummyOptimizer(lr=0.1)\n",
    "epochs = 100\n",
    "\n",
    "schedulers = {\n",
    "    \"StepLR\": StepLRScheduler(optimizer, initial_lr=0.1, step_size=20, gamma=0.5),\n",
    "    \"ExponentialLR\": ExponentialLRScheduler(optimizer, initial_lr=0.1, decay_rate=0.95),\n",
    "    \"CosineAnnealingLR\": CosineAnnealingScheduler(optimizer, initial_lr=0.1, T_max=100),\n",
    "}\n",
    "\n",
    "# Візуалізація\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "for name, scheduler in schedulers.items():\n",
    "    lrs = []\n",
    "    for _ in range(epochs):\n",
    "        lr = scheduler.step()\n",
    "        lrs.append(lr)\n",
    "    plt.plot(lrs, label=name)\n",
    "\n",
    "plt.title(\"Learning Rate Schedules\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb3d3ee6533f054",
   "metadata": {},
   "source": [
    "Частина 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559ad9369145c585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_finder(model, train_loader, start_lr=1e-7, end_lr=10):\n",
    "    \"\"\"\n",
    "    Реалізуйте LR finder:\n",
    "    1. Почніть з дуже малого LR\n",
    "    2. Поступово збільшуйте LR після кожного батчу\n",
    "    3. Записуйте loss для кожного LR\n",
    "    4. Зупиніться, коли loss почне вибухати\n",
    "    5. Знайдіть точку з найбільшим спадом (derivative)\n",
    "\n",
    "    Поверніть:\n",
    "    - список learning rates\n",
    "    - список losses\n",
    "    - рекомендований lr\n",
    "    \"\"\"\n",
    "\n",
    "    model_copy = copy.deepcopy(model)  # Не змінюємо оригінальну модель\n",
    "    optimizer = torch.optim.SGD(model_copy.parameters(), lr=start_lr)\n",
    "\n",
    "    lrs = []\n",
    "    losses = []\n",
    "\n",
    "    # Експоненційне збільшення lr\n",
    "    num_batches = len(train_loader)\n",
    "    lr_mult = (end_lr / start_lr) ** (1 / num_batches)\n",
    "\n",
    "    # Ваш код тут\n",
    "    # Підказка: слідкуйте за smoothed loss для стабільності\n",
    "\n",
    "    # Знайдіть оптимальний LR\n",
    "    # (точка з найбільшим негативним градієнтом на log scale)\n",
    "\n",
    "    return lrs, losses, optimal_lr\n",
    "\n",
    "# Візуалізуйте результати з логарифмічною шкалою для LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc3e2c493c51e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gradient_clipping():\n",
    "    \"\"\"\n",
    "    Експеримент з gradient clipping:\n",
    "    1. Створіть \"погану\" модель схильну до exploding gradients\n",
    "       (глибока мережа з поганою ініціалізацією)\n",
    "    2. Навчіть без clipping - покажіть проблему\n",
    "    3. Навчіть з різними значеннями max_norm (0.5, 1.0, 5.0, 10.0)\n",
    "    4. Порівняйте криві навчання\n",
    "    5. Знайдіть оптимальне значення max_norm\n",
    "    \"\"\"\n",
    "\n",
    "    # Створіть \"проблемну\" модель\n",
    "    class ProblematicModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            # Глибока мережа з великими початковими вагами\n",
    "            layers = []\n",
    "            for _ in range(10):  # 10 шарів!\n",
    "                layers.append(nn.Linear(50, 50))\n",
    "                layers.append(nn.ReLU())\n",
    "            self.network = nn.Sequential(*layers)\n",
    "\n",
    "            # Погана ініціалізація\n",
    "            for layer in self.network:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    layer.weight.data *= 10  # Занадто великі ваги!\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.network(x)\n",
    "\n",
    "    # Навчіть з різними max_norm\n",
    "    results = {}\n",
    "    for max_norm in [None, 0.5, 1.0, 5.0, 10.0]:\n",
    "        # Ваш код\n",
    "        # Зберігайте loss curves та gradient norms\n",
    "        pass\n",
    "\n",
    "    # Візуалізуйте порівняння\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a771a4c415077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overfitting_experiment():\n",
    "    \"\"\"\n",
    "    Створіть експеримент для демонстрації overfitting:\n",
    "\n",
    "    1. Використайте маленький датасет (100 прикладів)\n",
    "    2. Створіть занадто велику модель (1000+ параметрів)\n",
    "    3. Навчіть до повного overfitting\n",
    "    4. Застосуйте різні методи боротьби:\n",
    "       - Early stopping\n",
    "       - Зменшення розміру моделі\n",
    "       - Додавання шуму до даних\n",
    "    5. Порівняйте результати\n",
    "    \"\"\"\n",
    "\n",
    "    # Генеруємо маленький датасет\n",
    "    from sklearn.datasets import make_moons\n",
    "    X, y = make_moons(n_samples=100, noise=0.1, random_state=42)\n",
    "\n",
    "    # Занадто велика модель\n",
    "    class OverfittingModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.layers = nn.Sequential(\n",
    "                nn.Linear(2, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 50),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(50, 2)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.layers(x)\n",
    "\n",
    "    # Методи боротьби з overfitting\n",
    "    methods = {\n",
    "        'baseline': {},  # Без регуляризації\n",
    "        'early_stopping': {'patience': 5},\n",
    "        'smaller_model': {'hidden_size': 10},\n",
    "        'noise_injection': {'noise_std': 0.1}\n",
    "    }\n",
    "\n",
    "    # Навчіть і порівняйте\n",
    "    # Візуалізуйте decision boundaries\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d16bea1bd116309",
   "metadata": {},
   "source": [
    "Частина 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb4f9d0a2c53ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \"\"\"\n",
    "    Мінімальний autograd engine (як у micrograd Karpathy)\n",
    "    \"\"\"\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        # Реалізуйте множення з правильним backward pass\n",
    "        pass\n",
    "\n",
    "    def relu(self):\n",
    "        # Реалізуйте ReLU\n",
    "        pass\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"Backpropagation через граф\"\"\"\n",
    "        # Topological sort\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "# Тест: побудуйте просту мережу використовуючи Value\n",
    "# Порівняйте градієнти з PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d399eb8b0c29d83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLearner(nn.Module):\n",
    "    \"\"\"\n",
    "    Мета-мережа яка передбачає оптимальний learning rate\n",
    "    базуючись на поточному стані навчання\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_features=10):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()  # LR в діапазоні [0, 1]\n",
    "        )\n",
    "\n",
    "    def extract_features(self, model, loss_history, grad_history):\n",
    "        \"\"\"\n",
    "        Витягує ознаки поточного стану навчання:\n",
    "        - Поточний loss\n",
    "        - Зміна loss за останні k кроків\n",
    "        - Норма градієнтів\n",
    "        - Variance градієнтів\n",
    "        - Епоха\n",
    "        - тощо\n",
    "        \"\"\"\n",
    "        features = []\n",
    "\n",
    "        # Ваш код для витягування ознак\n",
    "        # Наприклад:\n",
    "        # features.append(loss_history[-1])\n",
    "        # features.append(np.std(loss_history[-10:]))\n",
    "        # ...\n",
    "\n",
    "        return torch.tensor(features)\n",
    "\n",
    "    def forward(self, features):\n",
    "        \"\"\"Передбачає оптимальний LR\"\"\"\n",
    "        return self.network(features) * 0.1  # Масштабуємо до [0, 0.1]\n",
    "\n",
    "def train_meta_learner():\n",
    "    \"\"\"\n",
    "    Навчіть meta-learner:\n",
    "    1. Навчіть багато моделей з різними фіксованими LR\n",
    "    2. Зберігайте який LR був оптимальним на кожному кроці\n",
    "    3. Навчіть meta-learner передбачати оптимальний LR\n",
    "    4. Протестуйте на новій задачі\n",
    "    \"\"\"\n",
    "    # Ваша реалізація\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Правило ланцюга\n",
    "\n",
    "$$ Якщо: y = f(g(x))$$\n",
    "$$\\frac{dy}{dx} = \\frac{dy}{dg} \\cdot \\frac{dg}{dx}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h_2} \\cdot \\frac{\\partial h_2}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial W_1}$$\n"
   ],
   "id": "2313b8f9dffc23c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Функція витрат\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "$$L = \\frac{1}{n} \\sum_{i=1}^{n} (y_{pred} - y_{true})^2$$\n",
    "\n",
    "### Binary Cross-Entropy\n",
    "$$L = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_{true} \\cdot \\log(y_{pred}) + (1-y_{true}) \\cdot \\log(1-y_{pred}) \\right]$$\n",
    "\n",
    "### Cross-Entropy\n",
    "$$L = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{C} y_{true}[i,j] \\cdot \\log(y_{pred}[i,j])$$\n",
    "\n"
   ],
   "id": "454c925868c510eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Backpropagation: покроковий алгоритм",
   "id": "8228ad3ad0b5130c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Крок 1\n",
    "def forward_pass(x, W1, b1, W2, b2):\n",
    "    # Шар 1\n",
    "    z1 = x @ W1 + b1\n",
    "    a1 = relu(z1)  # Зберігаємо z1 та a1!\n",
    "\n",
    "    # Шар 2\n",
    "    z2 = a1 @ W2 + b2\n",
    "    a2 = sigmoid(z2)  # Зберігаємо z2 та a2!\n",
    "\n",
    "    cache = {'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2}\n",
    "    return a2, cache\n",
    "\n",
    "#Крок 2\n",
    "def compute_loss(y_pred, y_true):\n",
    "    # Binary Cross-Entropy\n",
    "    loss = -np.mean(y_true * np.log(y_pred) +\n",
    "                    (1 - y_true) * np.log(1 - y_pred))  #np легко змінюється на torch\n",
    "    return loss\n",
    "\n",
    "#Крок 3\n",
    "def backward_pass(x, y_true, cache, W2):\n",
    "    m = x.shape[0]  # batch size\n",
    "\n",
    "    # Градієнт втрат по виходу\n",
    "    dz2 = cache['a2'] - y_true  # для BCE + sigmoid\n",
    "\n",
    "    # Градієнти для шару 2\n",
    "    dW2 = (1/m) * cache['a1'].T @ dz2\n",
    "    db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "    # Градієнт через шар 2\n",
    "    da1 = dz2 @ W2.T\n",
    "\n",
    "    # Градієнт через ReLU\n",
    "    dz1 = da1 * (cache['z1'] > 0)  # ReLU derivative\n",
    "\n",
    "    # Градієнти для шару 1\n",
    "    dW1 = (1/m) * x.T @ dz1\n",
    "    db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "    gradients = {'dW2': dW2, 'db2': db2, 'dW1': dW1, 'db1': db1}\n",
    "    return gradients"
   ],
   "id": "e056b38dd51145b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T21:02:44.271768Z",
     "start_time": "2025-09-29T21:02:44.266002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Графічне представлення потоку градієнтів\n",
    "def visualize_backprop():\n",
    "    \"\"\"\n",
    "    Показує як градієнт \"тече\" назад через мережу\n",
    "    \"\"\"\n",
    "    # Архітектура: 2 → 3 → 1\n",
    "\n",
    "    print(\"Forward Pass:\")\n",
    "    print(\"x → [W1] → h → [W2] → y → Loss\")\n",
    "    print(\"         ↓      ↓       ↓\")\n",
    "    print(\"    зберігаємо всі значення\")\n",
    "\n",
    "    print(\"\\nBackward Pass:\")\n",
    "    print(\"∂L/∂W1 ← ∂L/∂h ← ∂L/∂y ← ∂L/∂L=1\")\n",
    "    print(\"   ↑        ↑       ↑\")\n",
    "    print(\"chain    chain   chain\")\n",
    "    print(\"rule     rule    rule\")\n",
    "\n",
    "visualize_backprop()"
   ],
   "id": "b89f1e2c16e55fa6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Pass:\n",
      "x → [W1] → h → [W2] → y → Loss\n",
      "         ↓      ↓       ↓\n",
      "    зберігаємо всі значення\n",
      "\n",
      "Backward Pass:\n",
      "∂L/∂W1 ← ∂L/∂h ← ∂L/∂y ← ∂L/∂L=1\n",
      "   ↑        ↑       ↑\n",
      "chain    chain   chain\n",
      "rule     rule    rule\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Оптимізатори\n",
    "\n",
    "### Vanilla SGD\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla_{\\theta} L(\\theta_t)$$\n",
    "\n",
    "### SGD with Momentum\n",
    "$$v_{t+1} = \\beta \\cdot v_t - \\eta \\cdot \\nabla_{\\theta} L(\\theta_t)$$\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t + v_{t+1}$$\n",
    "\n",
    "### RMSprop\n",
    "$$s_{t+1} = \\beta \\cdot s_t + (1 - \\beta) \\cdot (\\nabla_{\\theta} L)^2$$\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{s_{t+1} + \\epsilon}} \\cdot \\nabla_{\\theta} L$$\n",
    "\n",
    "### Adam\n",
    "$$m_{t+1} = \\beta_1 \\cdot m_t + (1 - \\beta_1) \\cdot \\nabla_{\\theta} L$$\n",
    "\n",
    "$$v_{t+1} = \\beta_2 \\cdot v_t + (1 - \\beta_2) \\cdot (\\nabla_{\\theta} L)^2$$\n",
    "\n",
    "$$\\hat{m}_{t+1} = \\frac{m_{t+1}}{1 - \\beta_1^{t+1}}$$\n",
    "\n",
    "$$\\hat{v}_{t+1} = \\frac{v_{t+1}}{1 - \\beta_2^{t+1}}$$\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_{t+1}} + \\epsilon} \\cdot \\hat{m}_{t+1}$$\n",
    "\n"
   ],
   "id": "9958c0f7e77fbbf5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T21:16:16.378260Z",
     "start_time": "2025-09-29T21:16:16.372931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SGD:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.lr = learning_rate\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for param, grad in zip(params, grads):\n",
    "            param -= self.lr * grad\n",
    "\n",
    "class SGDMomentum:\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.velocity = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.velocity is None:\n",
    "            self.velocity = [np.zeros_like(p) for p in params]\n",
    "\n",
    "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
    "            self.velocity[i] = self.momentum * self.velocity[i] - self.lr * grad\n",
    "            param += self.velocity[i]\n",
    "\n",
    "class RMSprop:\n",
    "    def __init__(self, learning_rate=0.001, beta=0.9, epsilon=1e-8):\n",
    "        self.lr = learning_rate\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.cache = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.cache is None:\n",
    "            self.cache = [np.zeros_like(p) for p in params]\n",
    "\n",
    "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
    "            self.cache[i] = self.beta * self.cache[i] + (1 - self.beta) * grad**2\n",
    "            param -= self.lr * grad / (np.sqrt(self.cache[i]) + self.epsilon)\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m = [np.zeros_like(p) for p in params]\n",
    "            self.v = [np.zeros_like(p) for p in params]\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
    "            # Momentum\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
    "            # RMSprop\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * grad**2\n",
    "\n",
    "            # Bias correction\n",
    "            m_hat = self.m[i] / (1 - self.beta1**self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2**self.t)\n",
    "\n",
    "            # Update\n",
    "            param -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)"
   ],
   "id": "b4d3101ee9406f85",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4: Learning Rate Scheduling\n",
    "\n",
    "### Step Decay\n",
    "$$\\eta_t = \\eta_0 \\cdot \\gamma^{\\lfloor \\frac{t}{s} \\rfloor}$$\n",
    "де\n",
    "s - розмір кроку,\n",
    "γ - коефіцієнт зменшення\n",
    "\n",
    "### Exponential Decay\n",
    "$$\\eta_t = \\eta_0 \\cdot \\gamma^t$$\n",
    "\n",
    "### Cosine Annealing\n",
    "$$\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 + \\cos\\left(\\frac{\\pi \\cdot t}{T_{max}}\\right)\\right)$$\n",
    "\n",
    "### Linear Warmup\n",
    "$$\\eta_t = \\begin{cases}\n",
    "\\frac{t}{T_{warmup}} \\cdot \\eta_{target}, & \\text{if } t < T_{warmup} \\\\\n",
    "\\eta_{target}, & \\text{otherwise}\n",
    "\\end{cases}$$\n"
   ],
   "id": "f93c94edbd217231"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def find_learning_rate(model, data_loader, start_lr=1e-7, end_lr=10):\n",
    "    \"\"\"\n",
    "    Техніка для знаходження оптимального learning rate\n",
    "    \"\"\"\n",
    "    lrs = []\n",
    "    losses = []\n",
    "\n",
    "    lr = start_lr\n",
    "\n",
    "    for batch in data_loader:\n",
    "        # Forward pass\n",
    "        loss = model.compute_loss(batch)\n",
    "        losses.append(loss)\n",
    "        lrs.append(lr)\n",
    "\n",
    "        # Backward pass з поточним lr\n",
    "        model.backward()\n",
    "        model.update_weights(lr)\n",
    "\n",
    "        # Збільшуємо lr\n",
    "        lr *= 1.1\n",
    "\n",
    "        if lr > end_lr or loss > losses[0] * 4:\n",
    "            break\n",
    "\n",
    "    # Оптимальний lr - де loss падає найшвидше\n",
    "    # (найбільший нахил на графіку)\n",
    "    return lrs, losses"
   ],
   "id": "e4bf4012ab7bee5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Градієнти\n",
    "\n",
    "### Градієнт MSE\n",
    "$$\\frac{\\partial L_{MSE}}{\\partial y_{pred}} = \\frac{2}{n}(y_{pred} - y_{true})$$\n",
    "\n",
    "### Градієнт BCE\n",
    "$$\\frac{\\partial L_{BCE}}{\\partial y_{pred}} = -\\frac{1}{n}\\left(\\frac{y_{true}}{y_{pred}} - \\frac{1-y_{true}}{1-y_{pred}}\\right)$$\n",
    "\n",
    "### Градієнт Softmax + Cross-Entropy (спрощений)\n",
    "$$\\frac{\\partial L}{\\partial z_i} = y_{pred,i} - y_{true,i}$$\n",
    "\n",
    "### Gradient Clipping\n",
    "$$g_{clipped} = \\begin{cases}\n",
    "g, & \\text{if } ||g|| \\leq \\text{max\\_norm} \\\\\n",
    "\\frac{\\text{max\\_norm}}{||g||} \\cdot g, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n"
   ],
   "id": "451451d5019471f6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
